{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NSDistMultCE(nn.Module):\n",
        "  def __init__(self, in_dim=1024, embedding_dim=200, c_neg=None, regularization=0.002, device='cuda'):\n",
        "    super(NSDistMultCE, self).__init__()\n",
        "    self.in_dim         = in_dim\n",
        "    self.embedding_dim  = embedding_dim\n",
        "    self.c_neg          = c_neg\n",
        "    self.regularization = regularization\n",
        "    self.reduction      = torch.sum\n",
        "    self.device         = device\n",
        "\n",
        "    self.linear_emb   = nn.Linear(in_dim, embedding_dim, bias=False, device=self.device)\n",
        "    self.tanh         = nn.Tanh()\n",
        "    self.linear_score = nn.Linear(embedding_dim, 1, bias=False, device=self.device)\n",
        "    self.sigmoid      = nn.Sigmoid()\n",
        "    self.criterion    = nn.ReLU()\n",
        "  \n",
        "  def reset_parameters(self):\n",
        "    self.linear_emb.reset_parameters()\n",
        "    self.linear_score.reset_parameters()\n",
        "\n",
        "  def forward(self, embeddings, positive_pairs):\n",
        "    embeddings         = embeddings.squeeze().to(self.device)\n",
        "    low_dim_embeddings = self.embed(embeddings)\n",
        "    c_neg              = self.c_neg\n",
        "\n",
        "    if c_neg == None:\n",
        "      c_neg = len(positive_pairs[0]) / (embeddings.shape[0]**2 - len(positive_pairs[0]))\n",
        "\n",
        "      # Calculate L_p\n",
        "    positive_scores = self.get_score(low_dim_embeddings, positive_pairs)\n",
        "    L_p             = self.reduction(-torch.log(positive_scores) + c_neg * torch.log(1 - positive_scores), dtype=float)\n",
        "    del positive_scores\n",
        "\n",
        "      # Calculate L_\n",
        "    low_dim_embeddings_r = torch.einsum('i,ij->ij', self.linear_score.weight.squeeze(), low_dim_embeddings.T)\n",
        "    all_scores           = self.sigmoid(torch.mm(low_dim_embeddings, low_dim_embeddings_r))\n",
        "    L_a                  = -c_neg * self.reduction(torch.log(1 - all_scores), dtype=float)\n",
        "    del low_dim_embeddings_r\n",
        "    del all_scores\n",
        "    \n",
        "    return self.loss(L_p, L_a)\n",
        "\n",
        "  def embed(self, embeddings):\n",
        "    return self.tanh(self.linear_emb(embeddings))\n",
        "\n",
        "  def predict(self, embeddings, pairs):\n",
        "    low_dim_embeddings = self.embed(embeddings.to(self.device))\n",
        "    scores             = self.get_score(low_dim_embeddings, pairs)\n",
        "    return scores\n",
        "\n",
        "  def get_score(self, embeddings, pairs):\n",
        "    heads      = embeddings[pairs[0]]\n",
        "    tails      = embeddings[pairs[1]]\n",
        "    raw_scores = self.linear_score(heads * tails)\n",
        "    return self.sigmoid(raw_scores)\n",
        "\n",
        "  def loss(self, L_p, L_a):\n",
        "    return self.criterion((L_p + L_a) + self.regularization * (torch.norm(self.linear_score.weight) + torch.norm(self.linear_emb.weight)))"
      ],
      "metadata": {
        "id": "z_CysJtdseGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ProBertEmbeddings(Dataset):\n",
        "  \"\"\"ProBert Embeddings dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, data_path):\n",
        "    self.data_path      = data_path\n",
        "    self.organism_paths = self.get_organism_paths()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.organism_paths)\n",
        "\n",
        "  def get_organism_paths(self) -> list :\n",
        "    ids_path       = os.path.join(self.data_path, 'taxon_ids.txt')\n",
        "    organism_paths = list()\n",
        "    with open(ids_path) as handler:\n",
        "      for id in handler.readlines():\n",
        "        id           = id.strip()\n",
        "        current_path = os.path.join(self.data_path, id)\n",
        "        organism_paths.append(current_path)\n",
        "    return organism_paths\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    organism_path = self.organism_paths[idx]\n",
        "\n",
        "    embeddings     = torch.load(os.path.join(organism_path, 'embeddings.pt'))\n",
        "    positive_edges = torch.load(os.path.join(organism_path, 'positive_edges.pt'))\n",
        "\n",
        "    return embeddings, positive_edges"
      ],
      "metadata": {
        "id": "_eUd0tpZshCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataset, optimizer):\n",
        "  model.train()\n",
        "  loss_sum = 0\n",
        "  for i in range(len(dataset)):\n",
        "    embeddings, pairs = dataset[i]\n",
        "    pairs             = torch.tensor(pairs).T\n",
        "    loss              = model(embeddings, pairs)\n",
        "    loss_sum          = loss_sum + loss.to('cpu')\n",
        "  optimizer.zero_grad()\n",
        "  loss_sum.backward()\n",
        "  optimizer.step()\n",
        "  return loss_sum.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "  \n",
        "  N = 1000\n",
        "  samples = []\n",
        "  for emb, pairs in test_dataset:\n",
        "    neg_pairs = torch.randint(emb.shape[0], (N,2))\n",
        "    t = model.predict(emb.to('cuda'), torch.tensor(pairs).T).to('cpu').detach().flatten().numpy()\n",
        "    f = model.predict(emb.to('cuda'), neg_pairs.T).to('cpu').detach().flatten().numpy()\n",
        "    samples.append([t,f])\n",
        "  acc  = []\n",
        "  spec = []\n",
        "  for s in samples:\n",
        "    acc.append(sum(s[0] >= 0.5) / len(s[0]))\n",
        "    spec.append(sum(s[1] < 0.5) / len(s[1]))\n",
        "  return np.mean(acc), np.mean(spec)"
      ],
      "metadata": {
        "id": "J2lNSUp6sjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(mdoel, name):\n",
        "  torch.save(model.state_dict(), os.path.join(MAIN_PATH, name + '.pt'))"
      ],
      "metadata": {
        "id": "gYr8MayTslbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "MAIN_PATH = os.path.join('C:/Users/golde/Documents/bert2sage_data/Data')\n",
        "\n",
        "dataset = ProBertEmbeddings(MAIN_PATH)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [65, 4])"
      ],
      "metadata": {
        "id": "gRmZKECys6gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NSDistMultCE(embedding_dim=250, c_neg=None, device='cpu')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=0.004)\n",
        "\n",
        "time_list = []\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(1, 1200 + 1):\n",
        "  start_time = time.time()\n",
        "  loss_train = train(model, train_dataset, optimizer)\n",
        "  save_model(model, 'NSmodelCE3' + '_' + str(epoch))\n",
        "  acc, spec = test(model, test_dataset)\n",
        "  d_time = time.time() - start_time\n",
        "  time_list.append(d_time)\n",
        "  loss_list.append(loss_train)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss Train: {loss_train:.4f}, '\n",
        "        f'Accuracy: {acc:.4f}, '\n",
        "        f'Specificity: {spec:.4f}, '\n",
        "        f'Time: {d_time:f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nFMWD3nsqGo",
        "outputId": "436dd7c8-4ce7-4bc8-c39a-01f9da13d6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss Train: 6824017.1439, Accuracy: 0.8681, Specificity: 0.5238, Time: 36.589277\n",
            "Epoch: 02, Loss Train: 6817546.9796, Accuracy: 0.9029, Specificity: 0.4682, Time: 32.746373\n",
            "Epoch: 03, Loss Train: 6803711.0413, Accuracy: 0.9005, Specificity: 0.4887, Time: 32.392372\n",
            "Epoch: 04, Loss Train: 6780544.7237, Accuracy: 0.8936, Specificity: 0.4795, Time: 32.370415\n",
            "Epoch: 05, Loss Train: 6746518.6238, Accuracy: 0.8871, Specificity: 0.4948, Time: 32.285527\n",
            "Epoch: 06, Loss Train: 6700226.0227, Accuracy: 0.8813, Specificity: 0.5178, Time: 32.105030\n",
            "Epoch: 07, Loss Train: 6640741.3300, Accuracy: 0.8757, Specificity: 0.5178, Time: 32.296186\n",
            "Epoch: 08, Loss Train: 6567638.7276, Accuracy: 0.8700, Specificity: 0.5182, Time: 32.517242\n",
            "Epoch: 09, Loss Train: 6481096.7983, Accuracy: 0.8645, Specificity: 0.5325, Time: 32.607994\n",
            "Epoch: 10, Loss Train: 6381997.8599, Accuracy: 0.8575, Specificity: 0.5608, Time: 32.330018\n",
            "Epoch: 11, Loss Train: 6271972.4147, Accuracy: 0.8500, Specificity: 0.5490, Time: 32.316125\n",
            "Epoch: 12, Loss Train: 6153340.2936, Accuracy: 0.8413, Specificity: 0.5683, Time: 32.293484\n",
            "Epoch: 13, Loss Train: 6028929.4952, Accuracy: 0.8314, Specificity: 0.5787, Time: 33.667123\n",
            "Epoch: 14, Loss Train: 5901813.0838, Accuracy: 0.8190, Specificity: 0.6100, Time: 32.671213\n",
            "Epoch: 15, Loss Train: 5775109.1179, Accuracy: 0.8046, Specificity: 0.6355, Time: 32.365018\n",
            "Epoch: 16, Loss Train: 5651993.3395, Accuracy: 0.7899, Specificity: 0.6665, Time: 32.577635\n",
            "Epoch: 17, Loss Train: 5535704.3028, Accuracy: 0.7765, Specificity: 0.6865, Time: 33.092238\n",
            "Epoch: 18, Loss Train: 5429093.3014, Accuracy: 0.7667, Specificity: 0.6995, Time: 32.666982\n",
            "Epoch: 19, Loss Train: 5333633.6063, Accuracy: 0.7626, Specificity: 0.7140, Time: 32.398159\n",
            "Epoch: 20, Loss Train: 5248810.4111, Accuracy: 0.7647, Specificity: 0.7210, Time: 32.442842\n",
            "Epoch: 21, Loss Train: 5173036.5735, Accuracy: 0.7711, Specificity: 0.7205, Time: 32.573028\n",
            "Epoch: 22, Loss Train: 5105310.6350, Accuracy: 0.7784, Specificity: 0.7117, Time: 32.496800\n",
            "Epoch: 23, Loss Train: 5045385.3061, Accuracy: 0.7840, Specificity: 0.7077, Time: 32.602277\n",
            "Epoch: 24, Loss Train: 4992184.9611, Accuracy: 0.7854, Specificity: 0.7132, Time: 32.478830\n",
            "Epoch: 25, Loss Train: 4943562.7340, Accuracy: 0.7821, Specificity: 0.7195, Time: 32.523437\n",
            "Epoch: 26, Loss Train: 4898508.2023, Accuracy: 0.7780, Specificity: 0.7260, Time: 32.756763\n",
            "Epoch: 27, Loss Train: 4857630.9863, Accuracy: 0.7747, Specificity: 0.7332, Time: 32.230966\n",
            "Epoch: 28, Loss Train: 4820793.8739, Accuracy: 0.7765, Specificity: 0.7512, Time: 32.398083\n",
            "Epoch: 29, Loss Train: 4786857.9368, Accuracy: 0.7813, Specificity: 0.7415, Time: 32.439311\n",
            "Epoch: 30, Loss Train: 4756008.9359, Accuracy: 0.7867, Specificity: 0.7382, Time: 32.686581\n",
            "Epoch: 31, Loss Train: 4728937.7937, Accuracy: 0.7886, Specificity: 0.7340, Time: 32.522444\n",
            "Epoch: 32, Loss Train: 4704752.5202, Accuracy: 0.7867, Specificity: 0.7547, Time: 32.478180\n",
            "Epoch: 33, Loss Train: 4682705.5575, Accuracy: 0.7839, Specificity: 0.7362, Time: 32.532786\n",
            "Epoch: 34, Loss Train: 4663095.2803, Accuracy: 0.7833, Specificity: 0.7468, Time: 32.567317\n",
            "Epoch: 35, Loss Train: 4645167.1621, Accuracy: 0.7871, Specificity: 0.7660, Time: 32.507866\n",
            "Epoch: 36, Loss Train: 4628062.1607, Accuracy: 0.7918, Specificity: 0.7565, Time: 32.442923\n",
            "Epoch: 37, Loss Train: 4612151.0963, Accuracy: 0.7941, Specificity: 0.7417, Time: 32.564363\n",
            "Epoch: 38, Loss Train: 4597101.3114, Accuracy: 0.7932, Specificity: 0.7532, Time: 32.466019\n",
            "Epoch: 39, Loss Train: 4582479.8316, Accuracy: 0.7912, Specificity: 0.7605, Time: 32.504232\n",
            "Epoch: 40, Loss Train: 4568768.8782, Accuracy: 0.7919, Specificity: 0.7538, Time: 32.424725\n",
            "Epoch: 41, Loss Train: 4555698.1345, Accuracy: 0.7955, Specificity: 0.7602, Time: 32.449336\n",
            "Epoch: 42, Loss Train: 4543022.4921, Accuracy: 0.7998, Specificity: 0.7505, Time: 32.565861\n",
            "Epoch: 43, Loss Train: 4531068.4003, Accuracy: 0.8011, Specificity: 0.7648, Time: 32.891610\n",
            "Epoch: 44, Loss Train: 4519461.2109, Accuracy: 0.7999, Specificity: 0.7618, Time: 32.865093\n",
            "Epoch: 45, Loss Train: 4508118.0769, Accuracy: 0.7989, Specificity: 0.7510, Time: 32.478929\n",
            "Epoch: 46, Loss Train: 4497206.8946, Accuracy: 0.8005, Specificity: 0.7660, Time: 32.660778\n",
            "Epoch: 47, Loss Train: 4486430.8119, Accuracy: 0.8033, Specificity: 0.7555, Time: 32.536355\n",
            "Epoch: 48, Loss Train: 4475962.1107, Accuracy: 0.8047, Specificity: 0.7463, Time: 32.588505\n",
            "Epoch: 49, Loss Train: 4465897.8655, Accuracy: 0.8039, Specificity: 0.7485, Time: 32.706919\n",
            "Epoch: 50, Loss Train: 4456122.2782, Accuracy: 0.8030, Specificity: 0.7732, Time: 32.668601\n",
            "Epoch: 51, Loss Train: 4446884.8651, Accuracy: 0.8032, Specificity: 0.7685, Time: 32.648315\n",
            "Epoch: 52, Loss Train: 4438132.4147, Accuracy: 0.8052, Specificity: 0.7622, Time: 32.582870\n",
            "Epoch: 53, Loss Train: 4429851.1921, Accuracy: 0.8070, Specificity: 0.7518, Time: 32.418448\n",
            "Epoch: 54, Loss Train: 4422136.5071, Accuracy: 0.8068, Specificity: 0.7610, Time: 32.579324\n",
            "Epoch: 55, Loss Train: 4414799.0250, Accuracy: 0.8058, Specificity: 0.7525, Time: 32.445970\n",
            "Epoch: 56, Loss Train: 4407846.7492, Accuracy: 0.8059, Specificity: 0.7597, Time: 32.995058\n",
            "Epoch: 57, Loss Train: 4401164.9463, Accuracy: 0.8079, Specificity: 0.7488, Time: 34.659231\n",
            "Epoch: 58, Loss Train: 4394600.8190, Accuracy: 0.8099, Specificity: 0.7648, Time: 35.247851\n",
            "Epoch: 59, Loss Train: 4388156.4749, Accuracy: 0.8102, Specificity: 0.7603, Time: 34.176283\n",
            "Epoch: 60, Loss Train: 4381691.9529, Accuracy: 0.8096, Specificity: 0.7638, Time: 35.034504\n",
            "Epoch: 61, Loss Train: 4375242.9766, Accuracy: 0.8102, Specificity: 0.7675, Time: 34.933437\n",
            "Epoch: 62, Loss Train: 4368811.2723, Accuracy: 0.8122, Specificity: 0.7722, Time: 35.047900\n",
            "Epoch: 63, Loss Train: 4362406.6988, Accuracy: 0.8135, Specificity: 0.7492, Time: 34.471698\n",
            "Epoch: 64, Loss Train: 4356120.1193, Accuracy: 0.8138, Specificity: 0.7588, Time: 35.740736\n",
            "Epoch: 65, Loss Train: 4349927.9089, Accuracy: 0.8130, Specificity: 0.7665, Time: 35.012106\n",
            "Epoch: 66, Loss Train: 4343887.2587, Accuracy: 0.8134, Specificity: 0.7700, Time: 35.011097\n",
            "Epoch: 67, Loss Train: 4337953.0165, Accuracy: 0.8145, Specificity: 0.7690, Time: 34.896048\n",
            "Epoch: 68, Loss Train: 4332102.9110, Accuracy: 0.8151, Specificity: 0.7522, Time: 34.828753\n",
            "Epoch: 69, Loss Train: 4326303.4312, Accuracy: 0.8146, Specificity: 0.7552, Time: 34.807549\n",
            "Epoch: 70, Loss Train: 4320498.9367, Accuracy: 0.8139, Specificity: 0.7702, Time: 34.722384\n",
            "Epoch: 71, Loss Train: 4314696.7403, Accuracy: 0.8145, Specificity: 0.7643, Time: 34.828133\n",
            "Epoch: 72, Loss Train: 4308867.3945, Accuracy: 0.8152, Specificity: 0.7650, Time: 34.838075\n",
            "Epoch: 73, Loss Train: 4303054.6664, Accuracy: 0.8149, Specificity: 0.7680, Time: 34.894334\n",
            "Epoch: 74, Loss Train: 4297259.4120, Accuracy: 0.8142, Specificity: 0.7605, Time: 33.208493\n",
            "Epoch: 75, Loss Train: 4291520.7131, Accuracy: 0.8143, Specificity: 0.7728, Time: 32.634330\n",
            "Epoch: 76, Loss Train: 4285836.8012, Accuracy: 0.8152, Specificity: 0.7635, Time: 32.440255\n",
            "Epoch: 77, Loss Train: 4280209.4605, Accuracy: 0.8153, Specificity: 0.7607, Time: 32.612614\n",
            "Epoch: 78, Loss Train: 4274617.9557, Accuracy: 0.8149, Specificity: 0.7700, Time: 32.503903\n",
            "Epoch: 79, Loss Train: 4269038.1273, Accuracy: 0.8150, Specificity: 0.7695, Time: 32.396389\n",
            "Epoch: 80, Loss Train: 4263451.1855, Accuracy: 0.8158, Specificity: 0.7688, Time: 32.361368\n",
            "Epoch: 81, Loss Train: 4257838.9731, Accuracy: 0.8163, Specificity: 0.7712, Time: 32.421679\n",
            "Epoch: 82, Loss Train: 4252201.7515, Accuracy: 0.8163, Specificity: 0.7685, Time: 32.580361\n",
            "Epoch: 83, Loss Train: 4246535.1589, Accuracy: 0.8166, Specificity: 0.7680, Time: 32.364210\n",
            "Epoch: 84, Loss Train: 4240846.0241, Accuracy: 0.8174, Specificity: 0.7755, Time: 32.296788\n",
            "Epoch: 85, Loss Train: 4235126.3933, Accuracy: 0.8179, Specificity: 0.7792, Time: 32.863338\n",
            "Epoch: 86, Loss Train: 4229372.3656, Accuracy: 0.8180, Specificity: 0.7770, Time: 35.575119\n",
            "Epoch: 87, Loss Train: 4223566.7591, Accuracy: 0.8183, Specificity: 0.7720, Time: 35.116333\n",
            "Epoch: 88, Loss Train: 4217699.5886, Accuracy: 0.8190, Specificity: 0.7690, Time: 34.968809\n",
            "Epoch: 89, Loss Train: 4211758.5676, Accuracy: 0.8195, Specificity: 0.7743, Time: 34.672898\n",
            "Epoch: 90, Loss Train: 4205741.8762, Accuracy: 0.8195, Specificity: 0.7555, Time: 35.036308\n",
            "Epoch: 91, Loss Train: 4199650.1990, Accuracy: 0.8197, Specificity: 0.7728, Time: 34.954896\n",
            "Epoch: 92, Loss Train: 4193488.3563, Accuracy: 0.8203, Specificity: 0.7830, Time: 34.684858\n",
            "Epoch: 93, Loss Train: 4187261.2817, Accuracy: 0.8206, Specificity: 0.7720, Time: 34.713033\n",
            "Epoch: 94, Loss Train: 4180968.7506, Accuracy: 0.8208, Specificity: 0.7748, Time: 34.639521\n",
            "Epoch: 95, Loss Train: 4174610.6418, Accuracy: 0.8217, Specificity: 0.7778, Time: 34.915134\n",
            "Epoch: 96, Loss Train: 4168179.4538, Accuracy: 0.8223, Specificity: 0.7753, Time: 34.896198\n",
            "Epoch: 97, Loss Train: 4161671.3544, Accuracy: 0.8225, Specificity: 0.7788, Time: 34.638233\n",
            "Epoch: 98, Loss Train: 4155076.9407, Accuracy: 0.8227, Specificity: 0.7725, Time: 35.105811\n",
            "Epoch: 99, Loss Train: 4148392.8883, Accuracy: 0.8235, Specificity: 0.7692, Time: 34.781203\n",
            "Epoch: 100, Loss Train: 4141613.0010, Accuracy: 0.8241, Specificity: 0.7765, Time: 34.900842\n",
            "Epoch: 101, Loss Train: 4134734.6001, Accuracy: 0.8244, Specificity: 0.7760, Time: 35.099919\n",
            "Epoch: 102, Loss Train: 4127755.9567, Accuracy: 0.8249, Specificity: 0.7893, Time: 34.866838\n",
            "Epoch: 103, Loss Train: 4120676.4978, Accuracy: 0.8254, Specificity: 0.7810, Time: 35.000871\n",
            "Epoch: 104, Loss Train: 4113499.1240, Accuracy: 0.8259, Specificity: 0.7757, Time: 34.613188\n",
            "Epoch: 105, Loss Train: 4106225.3702, Accuracy: 0.8262, Specificity: 0.7778, Time: 34.509667\n",
            "Epoch: 106, Loss Train: 4098859.6924, Accuracy: 0.8270, Specificity: 0.7920, Time: 34.532370\n",
            "Epoch: 107, Loss Train: 4091404.5031, Accuracy: 0.8274, Specificity: 0.7828, Time: 35.209177\n",
            "Epoch: 108, Loss Train: 4083861.6800, Accuracy: 0.8275, Specificity: 0.7845, Time: 34.784171\n",
            "Epoch: 109, Loss Train: 4076233.7260, Accuracy: 0.8280, Specificity: 0.7805, Time: 34.923807\n",
            "Epoch: 110, Loss Train: 4068522.3707, Accuracy: 0.8286, Specificity: 0.7877, Time: 34.924059\n",
            "Epoch: 111, Loss Train: 4060731.6297, Accuracy: 0.8287, Specificity: 0.7820, Time: 34.822218\n",
            "Epoch: 112, Loss Train: 4052866.2469, Accuracy: 0.8291, Specificity: 0.7820, Time: 34.824609\n",
            "Epoch: 113, Loss Train: 4044931.9660, Accuracy: 0.8297, Specificity: 0.7825, Time: 34.836856\n",
            "Epoch: 114, Loss Train: 4036935.0385, Accuracy: 0.8302, Specificity: 0.7792, Time: 34.776605\n",
            "Epoch: 115, Loss Train: 4028880.3425, Accuracy: 0.8310, Specificity: 0.7918, Time: 34.716820\n",
            "Epoch: 116, Loss Train: 4020773.2118, Accuracy: 0.8317, Specificity: 0.7825, Time: 34.679647\n",
            "Epoch: 117, Loss Train: 4012619.1639, Accuracy: 0.8321, Specificity: 0.8010, Time: 34.712682\n",
            "Epoch: 118, Loss Train: 4004424.2396, Accuracy: 0.8326, Specificity: 0.7870, Time: 34.815351\n",
            "Epoch: 119, Loss Train: 3996197.0250, Accuracy: 0.8333, Specificity: 0.7878, Time: 34.494494\n",
            "Epoch: 120, Loss Train: 3987947.1335, Accuracy: 0.8339, Specificity: 0.7973, Time: 34.791404\n",
            "Epoch: 121, Loss Train: 3979684.3698, Accuracy: 0.8346, Specificity: 0.7863, Time: 34.800666\n",
            "Epoch: 122, Loss Train: 3971419.3334, Accuracy: 0.8353, Specificity: 0.7970, Time: 34.939624\n",
            "Epoch: 123, Loss Train: 3963161.8502, Accuracy: 0.8357, Specificity: 0.7975, Time: 34.758238\n",
            "Epoch: 124, Loss Train: 3954921.1723, Accuracy: 0.8364, Specificity: 0.7908, Time: 34.658812\n",
            "Epoch: 125, Loss Train: 3946707.5098, Accuracy: 0.8371, Specificity: 0.7875, Time: 34.816163\n",
            "Epoch: 126, Loss Train: 3938531.0828, Accuracy: 0.8376, Specificity: 0.7865, Time: 34.762078\n",
            "Epoch: 127, Loss Train: 3930401.9923, Accuracy: 0.8382, Specificity: 0.8003, Time: 34.786692\n",
            "Epoch: 128, Loss Train: 3922330.6500, Accuracy: 0.8387, Specificity: 0.8003, Time: 34.795766\n",
            "Epoch: 129, Loss Train: 3914326.3709, Accuracy: 0.8393, Specificity: 0.7940, Time: 34.959695\n",
            "Epoch: 130, Loss Train: 3906397.5497, Accuracy: 0.8400, Specificity: 0.8003, Time: 34.622462\n",
            "Epoch: 131, Loss Train: 3898552.2637, Accuracy: 0.8401, Specificity: 0.7965, Time: 34.669285\n",
            "Epoch: 132, Loss Train: 3890797.7641, Accuracy: 0.8410, Specificity: 0.7968, Time: 34.889921\n",
            "Epoch: 133, Loss Train: 3883140.9066, Accuracy: 0.8415, Specificity: 0.8017, Time: 34.874002\n",
            "Epoch: 134, Loss Train: 3875587.8233, Accuracy: 0.8420, Specificity: 0.8042, Time: 34.869154\n",
            "Epoch: 135, Loss Train: 3868143.3174, Accuracy: 0.8426, Specificity: 0.8055, Time: 34.987091\n",
            "Epoch: 136, Loss Train: 3860811.3508, Accuracy: 0.8431, Specificity: 0.8013, Time: 34.882857\n",
            "Epoch: 137, Loss Train: 3853594.6210, Accuracy: 0.8436, Specificity: 0.7975, Time: 34.914219\n",
            "Epoch: 138, Loss Train: 3846494.7622, Accuracy: 0.8439, Specificity: 0.8058, Time: 34.805157\n",
            "Epoch: 139, Loss Train: 3839512.7670, Accuracy: 0.8442, Specificity: 0.8010, Time: 34.722751\n",
            "Epoch: 140, Loss Train: 3832648.4787, Accuracy: 0.8445, Specificity: 0.7975, Time: 34.864240\n",
            "Epoch: 141, Loss Train: 3825900.8872, Accuracy: 0.8449, Specificity: 0.8097, Time: 34.844578\n",
            "Epoch: 142, Loss Train: 3819268.0703, Accuracy: 0.8451, Specificity: 0.7958, Time: 34.943513\n",
            "Epoch: 143, Loss Train: 3812747.2301, Accuracy: 0.8456, Specificity: 0.7980, Time: 34.720964\n",
            "Epoch: 144, Loss Train: 3806335.1791, Accuracy: 0.8459, Specificity: 0.8105, Time: 34.857559\n",
            "Epoch: 145, Loss Train: 3800028.1364, Accuracy: 0.8463, Specificity: 0.8102, Time: 34.908189\n",
            "Epoch: 146, Loss Train: 3793822.0524, Accuracy: 0.8467, Specificity: 0.8028, Time: 34.768196\n",
            "Epoch: 147, Loss Train: 3787712.6101, Accuracy: 0.8472, Specificity: 0.8065, Time: 34.692417\n",
            "Epoch: 148, Loss Train: 3781695.3303, Accuracy: 0.8474, Specificity: 0.8133, Time: 34.887929\n",
            "Epoch: 149, Loss Train: 3775765.9105, Accuracy: 0.8478, Specificity: 0.8073, Time: 35.177871\n",
            "Epoch: 150, Loss Train: 3769920.1280, Accuracy: 0.8481, Specificity: 0.8090, Time: 34.710064\n",
            "Epoch: 151, Loss Train: 3764154.1383, Accuracy: 0.8482, Specificity: 0.8090, Time: 34.576510\n",
            "Epoch: 152, Loss Train: 3758464.2709, Accuracy: 0.8485, Specificity: 0.8203, Time: 34.287009\n",
            "Epoch: 153, Loss Train: 3752847.2040, Accuracy: 0.8488, Specificity: 0.8107, Time: 34.993774\n",
            "Epoch: 154, Loss Train: 3747299.8902, Accuracy: 0.8490, Specificity: 0.7983, Time: 34.806128\n",
            "Epoch: 155, Loss Train: 3741819.6345, Accuracy: 0.8494, Specificity: 0.8080, Time: 34.913361\n",
            "Epoch: 156, Loss Train: 3736404.1217, Accuracy: 0.8498, Specificity: 0.8067, Time: 34.753977\n",
            "Epoch: 157, Loss Train: 3731051.3701, Accuracy: 0.8501, Specificity: 0.8150, Time: 34.804844\n",
            "Epoch: 158, Loss Train: 3725759.7709, Accuracy: 0.8503, Specificity: 0.8175, Time: 35.602753\n",
            "Epoch: 159, Loss Train: 3720527.9533, Accuracy: 0.8507, Specificity: 0.8132, Time: 35.558835\n",
            "Epoch: 160, Loss Train: 3715354.9088, Accuracy: 0.8510, Specificity: 0.8190, Time: 35.081722\n",
            "Epoch: 161, Loss Train: 3710239.7645, Accuracy: 0.8513, Specificity: 0.8075, Time: 35.069334\n",
            "Epoch: 162, Loss Train: 3705182.0077, Accuracy: 0.8514, Specificity: 0.8117, Time: 34.787934\n",
            "Epoch: 163, Loss Train: 3700181.1031, Accuracy: 0.8520, Specificity: 0.8018, Time: 34.856610\n",
            "Epoch: 164, Loss Train: 3695237.0886, Accuracy: 0.8517, Specificity: 0.7990, Time: 34.968390\n",
            "Epoch: 165, Loss Train: 3690350.1671, Accuracy: 0.8529, Specificity: 0.8095, Time: 35.041030\n",
            "Epoch: 166, Loss Train: 3685523.3767, Accuracy: 0.8518, Specificity: 0.8090, Time: 34.953987\n",
            "Epoch: 167, Loss Train: 3680765.2610, Accuracy: 0.8546, Specificity: 0.8087, Time: 34.593287\n",
            "Epoch: 168, Loss Train: 3676109.5259, Accuracy: 0.8505, Specificity: 0.8200, Time: 35.124868\n",
            "Epoch: 169, Loss Train: 3671658.3716, Accuracy: 0.8583, Specificity: 0.8137, Time: 34.980594\n",
            "Epoch: 170, Loss Train: 3667733.7267, Accuracy: 0.8449, Specificity: 0.8272, Time: 34.973083\n",
            "Epoch: 171, Loss Train: 3664900.0971, Accuracy: 0.8654, Specificity: 0.8063, Time: 35.228551\n",
            "Epoch: 172, Loss Train: 3663307.7755, Accuracy: 0.8408, Specificity: 0.8233, Time: 34.945566\n",
            "Epoch: 173, Loss Train: 3659417.1563, Accuracy: 0.8613, Specificity: 0.8102, Time: 34.866966\n",
            "Epoch: 174, Loss Train: 3651614.1777, Accuracy: 0.8558, Specificity: 0.8017, Time: 34.972951\n",
            "Epoch: 175, Loss Train: 3645558.2633, Accuracy: 0.8459, Specificity: 0.8270, Time: 35.034118\n",
            "Epoch: 176, Loss Train: 3644226.5826, Accuracy: 0.8641, Specificity: 0.7973, Time: 34.946410\n",
            "Epoch: 177, Loss Train: 3641071.6474, Accuracy: 0.8515, Specificity: 0.8272, Time: 34.856806\n",
            "Epoch: 178, Loss Train: 3634275.1652, Accuracy: 0.8512, Specificity: 0.8397, Time: 34.791422\n",
            "Epoch: 179, Loss Train: 3630610.4200, Accuracy: 0.8634, Specificity: 0.8153, Time: 35.091889\n",
            "Epoch: 180, Loss Train: 3628802.9697, Accuracy: 0.8501, Specificity: 0.8255, Time: 35.177225\n",
            "Epoch: 181, Loss Train: 3623624.8033, Accuracy: 0.8547, Specificity: 0.8137, Time: 35.176154\n",
            "Epoch: 182, Loss Train: 3618827.0735, Accuracy: 0.8622, Specificity: 0.8095, Time: 35.071566\n",
            "Epoch: 183, Loss Train: 3616688.1490, Accuracy: 0.8504, Specificity: 0.8277, Time: 34.903760\n",
            "Epoch: 184, Loss Train: 3612846.0185, Accuracy: 0.8569, Specificity: 0.8183, Time: 35.010315\n",
            "Epoch: 185, Loss Train: 3607951.2815, Accuracy: 0.8609, Specificity: 0.8193, Time: 35.111729\n",
            "Epoch: 186, Loss Train: 3605146.1452, Accuracy: 0.8511, Specificity: 0.8315, Time: 35.112278\n",
            "Epoch: 187, Loss Train: 3602058.3739, Accuracy: 0.8587, Specificity: 0.8243, Time: 35.173065\n",
            "Epoch: 188, Loss Train: 3597533.5863, Accuracy: 0.8594, Specificity: 0.8208, Time: 35.109317\n",
            "Epoch: 189, Loss Train: 3594152.3485, Accuracy: 0.8525, Specificity: 0.8300, Time: 35.020935\n",
            "Epoch: 190, Loss Train: 3591358.7050, Accuracy: 0.8602, Specificity: 0.8133, Time: 34.927812\n",
            "Epoch: 191, Loss Train: 3587369.8585, Accuracy: 0.8585, Specificity: 0.8237, Time: 34.938534\n",
            "Epoch: 192, Loss Train: 3583629.3475, Accuracy: 0.8540, Specificity: 0.8285, Time: 34.990819\n",
            "Epoch: 193, Loss Train: 3580797.7931, Accuracy: 0.8613, Specificity: 0.8233, Time: 34.930069\n",
            "Epoch: 194, Loss Train: 3577335.5994, Accuracy: 0.8576, Specificity: 0.8267, Time: 34.994316\n",
            "Epoch: 195, Loss Train: 3573504.7513, Accuracy: 0.8557, Specificity: 0.8263, Time: 34.705656\n",
            "Epoch: 196, Loss Train: 3570430.9958, Accuracy: 0.8620, Specificity: 0.8172, Time: 35.012416\n",
            "Epoch: 197, Loss Train: 3567341.2770, Accuracy: 0.8571, Specificity: 0.8330, Time: 35.046937\n",
            "Epoch: 198, Loss Train: 3563684.4438, Accuracy: 0.8579, Specificity: 0.8097, Time: 34.953806\n",
            "Epoch: 199, Loss Train: 3560335.7618, Accuracy: 0.8617, Specificity: 0.8110, Time: 35.128738\n",
            "Epoch: 200, Loss Train: 3557359.9897, Accuracy: 0.8568, Specificity: 0.8180, Time: 35.091660\n",
            "Epoch: 201, Loss Train: 3554030.2775, Accuracy: 0.8597, Specificity: 0.8225, Time: 35.011452\n",
            "Epoch: 202, Loss Train: 3550565.5784, Accuracy: 0.8611, Specificity: 0.8275, Time: 35.033805\n",
            "Epoch: 203, Loss Train: 3547464.9607, Accuracy: 0.8571, Specificity: 0.8385, Time: 34.931649\n",
            "Epoch: 204, Loss Train: 3544400.7890, Accuracy: 0.8613, Specificity: 0.8233, Time: 34.823519\n",
            "Epoch: 205, Loss Train: 3541067.8205, Accuracy: 0.8602, Specificity: 0.8098, Time: 35.017828\n",
            "Epoch: 206, Loss Train: 3537798.1875, Accuracy: 0.8579, Specificity: 0.8243, Time: 35.180339\n",
            "Epoch: 207, Loss Train: 3534764.3319, Accuracy: 0.8622, Specificity: 0.8140, Time: 35.059795\n",
            "Epoch: 208, Loss Train: 3531667.9713, Accuracy: 0.8588, Specificity: 0.8260, Time: 34.828582\n",
            "Epoch: 209, Loss Train: 3528416.6528, Accuracy: 0.8598, Specificity: 0.8162, Time: 35.045858\n",
            "Epoch: 210, Loss Train: 3525259.7710, Accuracy: 0.8621, Specificity: 0.8335, Time: 34.960123\n",
            "Epoch: 211, Loss Train: 3522245.2692, Accuracy: 0.8586, Specificity: 0.8232, Time: 34.821420\n",
            "Epoch: 212, Loss Train: 3519174.4259, Accuracy: 0.8616, Specificity: 0.8190, Time: 35.045797\n",
            "Epoch: 213, Loss Train: 3516013.8234, Accuracy: 0.8610, Specificity: 0.8280, Time: 35.146186\n",
            "Epoch: 214, Loss Train: 3512916.7624, Accuracy: 0.8594, Specificity: 0.8260, Time: 34.989884\n",
            "Epoch: 215, Loss Train: 3509918.5086, Accuracy: 0.8623, Specificity: 0.8245, Time: 34.344784\n",
            "Epoch: 216, Loss Train: 3506901.8066, Accuracy: 0.8601, Specificity: 0.8253, Time: 34.777384\n",
            "Epoch: 217, Loss Train: 3503823.1677, Accuracy: 0.8612, Specificity: 0.8312, Time: 34.860245\n",
            "Epoch: 218, Loss Train: 3500767.2789, Accuracy: 0.8619, Specificity: 0.8322, Time: 35.042848\n",
            "Epoch: 219, Loss Train: 3497784.0031, Accuracy: 0.8599, Specificity: 0.8260, Time: 34.848823\n",
            "Epoch: 220, Loss Train: 3494820.0914, Accuracy: 0.8623, Specificity: 0.8307, Time: 35.008729\n",
            "Epoch: 221, Loss Train: 3491822.0060, Accuracy: 0.8609, Specificity: 0.8277, Time: 34.399839\n",
            "Epoch: 222, Loss Train: 3488812.6985, Accuracy: 0.8613, Specificity: 0.8247, Time: 34.887270\n",
            "Epoch: 223, Loss Train: 3485841.8597, Accuracy: 0.8626, Specificity: 0.8232, Time: 35.186344\n",
            "Epoch: 224, Loss Train: 3482911.2255, Accuracy: 0.8609, Specificity: 0.8212, Time: 35.139318\n",
            "Epoch: 225, Loss Train: 3479983.6089, Accuracy: 0.8630, Specificity: 0.8365, Time: 35.148076\n",
            "Epoch: 226, Loss Train: 3477039.6489, Accuracy: 0.8617, Specificity: 0.8307, Time: 35.216611\n",
            "Epoch: 227, Loss Train: 3474096.8091, Accuracy: 0.8621, Specificity: 0.8347, Time: 34.849586\n",
            "Epoch: 228, Loss Train: 3471179.5485, Accuracy: 0.8631, Specificity: 0.8313, Time: 35.307294\n",
            "Epoch: 229, Loss Train: 3468289.1723, Accuracy: 0.8618, Specificity: 0.8300, Time: 35.156597\n",
            "Epoch: 230, Loss Train: 3465408.1303, Accuracy: 0.8636, Specificity: 0.8162, Time: 34.827433\n",
            "Epoch: 231, Loss Train: 3462523.1711, Accuracy: 0.8624, Specificity: 0.8295, Time: 35.060168\n",
            "Epoch: 232, Loss Train: 3459637.2277, Accuracy: 0.8632, Specificity: 0.8287, Time: 35.135822\n",
            "Epoch: 233, Loss Train: 3456762.1661, Accuracy: 0.8633, Specificity: 0.8375, Time: 35.037338\n",
            "Epoch: 234, Loss Train: 3453905.1806, Accuracy: 0.8629, Specificity: 0.8177, Time: 34.965101\n",
            "Epoch: 235, Loss Train: 3451063.6808, Accuracy: 0.8640, Specificity: 0.8257, Time: 34.887483\n",
            "Epoch: 236, Loss Train: 3448230.0130, Accuracy: 0.8630, Specificity: 0.8263, Time: 34.845187\n",
            "Epoch: 237, Loss Train: 3445398.6909, Accuracy: 0.8642, Specificity: 0.8232, Time: 34.940512\n",
            "Epoch: 238, Loss Train: 3442569.5755, Accuracy: 0.8634, Specificity: 0.8237, Time: 34.971139\n",
            "Epoch: 239, Loss Train: 3439746.1270, Accuracy: 0.8641, Specificity: 0.8402, Time: 35.015084\n",
            "Epoch: 240, Loss Train: 3436931.9275, Accuracy: 0.8642, Specificity: 0.8315, Time: 35.166458\n",
            "Epoch: 241, Loss Train: 3434128.3888, Accuracy: 0.8642, Specificity: 0.8347, Time: 34.961483\n",
            "Epoch: 242, Loss Train: 3431334.6060, Accuracy: 0.8648, Specificity: 0.8325, Time: 34.931493\n",
            "Epoch: 243, Loss Train: 3428548.5379, Accuracy: 0.8642, Specificity: 0.8257, Time: 34.898767\n",
            "Epoch: 244, Loss Train: 3425768.3100, Accuracy: 0.8652, Specificity: 0.8260, Time: 35.317132\n",
            "Epoch: 245, Loss Train: 3422992.8491, Accuracy: 0.8644, Specificity: 0.8308, Time: 35.218612\n",
            "Epoch: 246, Loss Train: 3420221.9009, Accuracy: 0.8655, Specificity: 0.8335, Time: 34.978275\n",
            "Epoch: 247, Loss Train: 3417455.7222, Accuracy: 0.8647, Specificity: 0.8267, Time: 34.961932\n",
            "Epoch: 248, Loss Train: 3414694.6949, Accuracy: 0.8656, Specificity: 0.8380, Time: 34.951014\n",
            "Epoch: 249, Loss Train: 3411939.1146, Accuracy: 0.8650, Specificity: 0.8347, Time: 34.917616\n",
            "Epoch: 250, Loss Train: 3409189.1078, Accuracy: 0.8657, Specificity: 0.8310, Time: 35.063831\n",
            "Epoch: 251, Loss Train: 3406444.6479, Accuracy: 0.8652, Specificity: 0.8377, Time: 35.110178\n",
            "Epoch: 252, Loss Train: 3403705.6371, Accuracy: 0.8659, Specificity: 0.8365, Time: 35.069527\n",
            "Epoch: 253, Loss Train: 3400971.9528, Accuracy: 0.8654, Specificity: 0.8207, Time: 35.016017\n",
            "Epoch: 254, Loss Train: 3398243.5371, Accuracy: 0.8662, Specificity: 0.8345, Time: 35.300878\n",
            "Epoch: 255, Loss Train: 3395520.4315, Accuracy: 0.8656, Specificity: 0.8277, Time: 35.072304\n",
            "Epoch: 256, Loss Train: 3392802.9137, Accuracy: 0.8666, Specificity: 0.8297, Time: 35.080520\n",
            "Epoch: 257, Loss Train: 3390091.6852, Accuracy: 0.8656, Specificity: 0.8402, Time: 35.012054\n",
            "Epoch: 258, Loss Train: 3387388.2942, Accuracy: 0.8674, Specificity: 0.8220, Time: 34.899762\n",
            "Epoch: 259, Loss Train: 3384696.1691, Accuracy: 0.8654, Specificity: 0.8408, Time: 35.250118\n",
            "Epoch: 260, Loss Train: 3382022.5667, Accuracy: 0.8684, Specificity: 0.8282, Time: 34.956641\n",
            "Epoch: 261, Loss Train: 3379383.9955, Accuracy: 0.8645, Specificity: 0.8380, Time: 35.018978\n",
            "Epoch: 262, Loss Train: 3376815.8512, Accuracy: 0.8705, Specificity: 0.8310, Time: 35.034945\n",
            "Epoch: 263, Loss Train: 3374400.9028, Accuracy: 0.8621, Specificity: 0.8425, Time: 35.165402\n",
            "Epoch: 264, Loss Train: 3372305.8337, Accuracy: 0.8746, Specificity: 0.8185, Time: 35.195711\n",
            "Epoch: 265, Loss Train: 3370884.4870, Accuracy: 0.8569, Specificity: 0.8525, Time: 35.101439\n",
            "Epoch: 266, Loss Train: 3370511.6612, Accuracy: 0.8807, Specificity: 0.8220, Time: 34.929930\n",
            "Epoch: 267, Loss Train: 3371193.9667, Accuracy: 0.8523, Specificity: 0.8512, Time: 34.989173\n",
            "Epoch: 268, Loss Train: 3370124.7986, Accuracy: 0.8796, Specificity: 0.8107, Time: 35.186618\n",
            "Epoch: 269, Loss Train: 3364582.0646, Accuracy: 0.8633, Specificity: 0.8430, Time: 35.116348\n",
            "Epoch: 270, Loss Train: 3356847.5536, Accuracy: 0.8642, Specificity: 0.8340, Time: 35.085747\n",
            "Epoch: 271, Loss Train: 3354081.2213, Accuracy: 0.8779, Specificity: 0.8257, Time: 35.137240\n",
            "Epoch: 272, Loss Train: 3355272.3206, Accuracy: 0.8575, Specificity: 0.8450, Time: 35.303236\n",
            "Epoch: 273, Loss Train: 3353306.9110, Accuracy: 0.8739, Specificity: 0.8375, Time: 35.007225\n",
            "Epoch: 274, Loss Train: 3347488.5243, Accuracy: 0.8707, Specificity: 0.8520, Time: 35.148506\n",
            "Epoch: 275, Loss Train: 3344046.2883, Accuracy: 0.8610, Specificity: 0.8462, Time: 35.073533\n",
            "Epoch: 276, Loss Train: 3344105.9076, Accuracy: 0.8771, Specificity: 0.8288, Time: 34.979432\n",
            "Epoch: 277, Loss Train: 3342175.3229, Accuracy: 0.8649, Specificity: 0.8285, Time: 35.305157\n",
            "Epoch: 278, Loss Train: 3337412.5178, Accuracy: 0.8663, Specificity: 0.8453, Time: 35.195506\n",
            "Epoch: 279, Loss Train: 3334761.2170, Accuracy: 0.8759, Specificity: 0.8242, Time: 34.329726\n",
            "Epoch: 280, Loss Train: 3334168.7024, Accuracy: 0.8628, Specificity: 0.8547, Time: 35.052498\n",
            "Epoch: 281, Loss Train: 3331550.7127, Accuracy: 0.8715, Specificity: 0.8307, Time: 35.025717\n",
            "Epoch: 282, Loss Train: 3327657.1936, Accuracy: 0.8731, Specificity: 0.8337, Time: 34.988812\n",
            "Epoch: 283, Loss Train: 3325665.3583, Accuracy: 0.8634, Specificity: 0.8425, Time: 35.208756\n",
            "Epoch: 284, Loss Train: 3324409.1295, Accuracy: 0.8744, Specificity: 0.8365, Time: 35.166407\n",
            "Epoch: 285, Loss Train: 3321509.8643, Accuracy: 0.8696, Specificity: 0.8447, Time: 35.254058\n",
            "Epoch: 286, Loss Train: 3318323.5605, Accuracy: 0.8666, Specificity: 0.8487, Time: 35.255272\n",
            "Epoch: 287, Loss Train: 3316578.4862, Accuracy: 0.8753, Specificity: 0.8335, Time: 34.910598\n",
            "Epoch: 288, Loss Train: 3314819.5993, Accuracy: 0.8672, Specificity: 0.8468, Time: 35.450308\n",
            "Epoch: 289, Loss Train: 3311898.2868, Accuracy: 0.8701, Specificity: 0.8377, Time: 35.104766\n",
            "Epoch: 290, Loss Train: 3309216.6483, Accuracy: 0.8740, Specificity: 0.8293, Time: 35.092681\n",
            "Epoch: 291, Loss Train: 3307448.5553, Accuracy: 0.8665, Specificity: 0.8457, Time: 35.154870\n",
            "Epoch: 292, Loss Train: 3305366.6389, Accuracy: 0.8730, Specificity: 0.8317, Time: 34.785597\n",
            "Epoch: 293, Loss Train: 3302608.1807, Accuracy: 0.8720, Specificity: 0.8493, Time: 34.958899\n",
            "Epoch: 294, Loss Train: 3300192.0157, Accuracy: 0.8679, Specificity: 0.8450, Time: 35.040037\n",
            "Epoch: 295, Loss Train: 3298304.2863, Accuracy: 0.8744, Specificity: 0.8277, Time: 34.934983\n",
            "Epoch: 296, Loss Train: 3296105.8299, Accuracy: 0.8699, Specificity: 0.8380, Time: 35.118773\n",
            "Epoch: 297, Loss Train: 3293507.2206, Accuracy: 0.8706, Specificity: 0.8457, Time: 35.176541\n",
            "Epoch: 298, Loss Train: 3291194.9784, Accuracy: 0.8741, Specificity: 0.8362, Time: 35.088821\n",
            "Epoch: 299, Loss Train: 3289201.7274, Accuracy: 0.8691, Specificity: 0.8435, Time: 34.967267\n",
            "Epoch: 300, Loss Train: 3286980.1995, Accuracy: 0.8731, Specificity: 0.8420, Time: 34.809122\n",
            "Epoch: 301, Loss Train: 3284509.9527, Accuracy: 0.8725, Specificity: 0.8462, Time: 35.446862\n",
            "Epoch: 302, Loss Train: 3282221.0413, Accuracy: 0.8701, Specificity: 0.8395, Time: 35.157721\n",
            "Epoch: 303, Loss Train: 3280149.0789, Accuracy: 0.8744, Specificity: 0.8360, Time: 35.022286\n",
            "Epoch: 304, Loss Train: 3277953.6953, Accuracy: 0.8709, Specificity: 0.8468, Time: 35.375635\n",
            "Epoch: 305, Loss Train: 3275575.6374, Accuracy: 0.8721, Specificity: 0.8377, Time: 34.986932\n",
            "Epoch: 306, Loss Train: 3273281.5109, Accuracy: 0.8740, Specificity: 0.8362, Time: 35.118170\n",
            "Epoch: 307, Loss Train: 3271149.5957, Accuracy: 0.8707, Specificity: 0.8440, Time: 35.205883\n",
            "Epoch: 308, Loss Train: 3268982.0321, Accuracy: 0.8741, Specificity: 0.8407, Time: 35.020342\n",
            "Epoch: 309, Loss Train: 3266682.5681, Accuracy: 0.8726, Specificity: 0.8435, Time: 34.951080\n",
            "Epoch: 310, Loss Train: 3264385.2630, Accuracy: 0.8720, Specificity: 0.8405, Time: 35.031238\n",
            "Epoch: 311, Loss Train: 3262198.2796, Accuracy: 0.8745, Specificity: 0.8365, Time: 35.019449\n",
            "Epoch: 312, Loss Train: 3260042.6792, Accuracy: 0.8717, Specificity: 0.8390, Time: 35.200520\n",
            "Epoch: 313, Loss Train: 3257810.9789, Accuracy: 0.8738, Specificity: 0.8375, Time: 34.932146\n",
            "Epoch: 314, Loss Train: 3255532.8733, Accuracy: 0.8735, Specificity: 0.8343, Time: 35.019756\n",
            "Epoch: 315, Loss Train: 3253301.0361, Accuracy: 0.8724, Specificity: 0.8385, Time: 35.048239\n",
            "Epoch: 316, Loss Train: 3251126.5230, Accuracy: 0.8749, Specificity: 0.8433, Time: 35.081697\n",
            "Epoch: 317, Loss Train: 3248941.1222, Accuracy: 0.8727, Specificity: 0.8387, Time: 35.270900\n",
            "Epoch: 318, Loss Train: 3246707.1454, Accuracy: 0.8742, Specificity: 0.8530, Time: 35.087949\n",
            "Epoch: 319, Loss Train: 3244461.5621, Accuracy: 0.8743, Specificity: 0.8365, Time: 34.856819\n",
            "Epoch: 320, Loss Train: 3242250.2129, Accuracy: 0.8733, Specificity: 0.8453, Time: 35.151223\n",
            "Epoch: 321, Loss Train: 3240067.8943, Accuracy: 0.8753, Specificity: 0.8400, Time: 35.093813\n",
            "Epoch: 322, Loss Train: 3237876.3594, Accuracy: 0.8737, Specificity: 0.8425, Time: 35.088454\n",
            "Epoch: 323, Loss Train: 3235657.7210, Accuracy: 0.8749, Specificity: 0.8382, Time: 35.176637\n",
            "Epoch: 324, Loss Train: 3233431.4171, Accuracy: 0.8748, Specificity: 0.8438, Time: 35.167634\n",
            "Epoch: 325, Loss Train: 3231222.5372, Accuracy: 0.8742, Specificity: 0.8367, Time: 35.048065\n",
            "Epoch: 326, Loss Train: 3229032.7663, Accuracy: 0.8756, Specificity: 0.8420, Time: 35.210848\n",
            "Epoch: 327, Loss Train: 3226844.0000, Accuracy: 0.8742, Specificity: 0.8375, Time: 34.948295\n",
            "Epoch: 328, Loss Train: 3224641.8576, Accuracy: 0.8756, Specificity: 0.8442, Time: 35.126396\n",
            "Epoch: 329, Loss Train: 3222429.8805, Accuracy: 0.8751, Specificity: 0.8438, Time: 35.219554\n",
            "Epoch: 330, Loss Train: 3220221.1141, Accuracy: 0.8753, Specificity: 0.8432, Time: 34.891353\n",
            "Epoch: 331, Loss Train: 3218023.6513, Accuracy: 0.8759, Specificity: 0.8382, Time: 34.966892\n",
            "Epoch: 332, Loss Train: 3215834.1444, Accuracy: 0.8751, Specificity: 0.8357, Time: 35.230664\n",
            "Epoch: 333, Loss Train: 3213643.6175, Accuracy: 0.8764, Specificity: 0.8425, Time: 35.057547\n",
            "Epoch: 334, Loss Train: 3211446.7953, Accuracy: 0.8754, Specificity: 0.8380, Time: 34.938844\n",
            "Epoch: 335, Loss Train: 3209245.1114, Accuracy: 0.8762, Specificity: 0.8470, Time: 35.192479\n",
            "Epoch: 336, Loss Train: 3207044.1909, Accuracy: 0.8761, Specificity: 0.8305, Time: 35.272357\n",
            "Epoch: 337, Loss Train: 3204848.1088, Accuracy: 0.8761, Specificity: 0.8488, Time: 35.316107\n",
            "Epoch: 338, Loss Train: 3202657.0274, Accuracy: 0.8769, Specificity: 0.8362, Time: 35.082743\n",
            "Epoch: 339, Loss Train: 3200468.1017, Accuracy: 0.8762, Specificity: 0.8507, Time: 35.085074\n",
            "Epoch: 340, Loss Train: 3198278.0661, Accuracy: 0.8772, Specificity: 0.8365, Time: 35.147465\n",
            "Epoch: 341, Loss Train: 3196085.6147, Accuracy: 0.8766, Specificity: 0.8478, Time: 35.055071\n",
            "Epoch: 342, Loss Train: 3193891.2645, Accuracy: 0.8773, Specificity: 0.8362, Time: 35.005360\n",
            "Epoch: 343, Loss Train: 3191696.8526, Accuracy: 0.8770, Specificity: 0.8488, Time: 35.279080\n",
            "Epoch: 344, Loss Train: 3189503.8491, Accuracy: 0.8771, Specificity: 0.8360, Time: 34.979009\n",
            "Epoch: 345, Loss Train: 3187312.9597, Accuracy: 0.8775, Specificity: 0.8372, Time: 35.411999\n",
            "Epoch: 346, Loss Train: 3185123.9149, Accuracy: 0.8771, Specificity: 0.8448, Time: 34.087128\n",
            "Epoch: 347, Loss Train: 3182935.9180, Accuracy: 0.8779, Specificity: 0.8412, Time: 34.915303\n",
            "Epoch: 348, Loss Train: 3180748.2375, Accuracy: 0.8773, Specificity: 0.8515, Time: 35.065809\n",
            "Epoch: 349, Loss Train: 3178560.2705, Accuracy: 0.8782, Specificity: 0.8433, Time: 35.083047\n",
            "Epoch: 350, Loss Train: 3176371.9336, Accuracy: 0.8775, Specificity: 0.8432, Time: 35.320776\n",
            "Epoch: 351, Loss Train: 3174183.2152, Accuracy: 0.8782, Specificity: 0.8395, Time: 35.174807\n",
            "Epoch: 352, Loss Train: 3171994.4381, Accuracy: 0.8777, Specificity: 0.8458, Time: 34.916132\n",
            "Epoch: 353, Loss Train: 3169805.7251, Accuracy: 0.8784, Specificity: 0.8410, Time: 35.344908\n",
            "Epoch: 354, Loss Train: 3167617.3424, Accuracy: 0.8780, Specificity: 0.8403, Time: 35.230725\n",
            "Epoch: 355, Loss Train: 3165429.2967, Accuracy: 0.8785, Specificity: 0.8415, Time: 35.124774\n",
            "Epoch: 356, Loss Train: 3163241.6927, Accuracy: 0.8784, Specificity: 0.8492, Time: 35.098589\n",
            "Epoch: 357, Loss Train: 3161054.4525, Accuracy: 0.8788, Specificity: 0.8417, Time: 34.855171\n",
            "Epoch: 358, Loss Train: 3158867.5982, Accuracy: 0.8788, Specificity: 0.8562, Time: 35.151976\n",
            "Epoch: 359, Loss Train: 3156681.0480, Accuracy: 0.8791, Specificity: 0.8470, Time: 35.100134\n",
            "Epoch: 360, Loss Train: 3154494.8103, Accuracy: 0.8789, Specificity: 0.8377, Time: 34.744909\n",
            "Epoch: 361, Loss Train: 3152308.8288, Accuracy: 0.8793, Specificity: 0.8505, Time: 35.146592\n",
            "Epoch: 362, Loss Train: 3150123.1332, Accuracy: 0.8792, Specificity: 0.8512, Time: 34.868102\n",
            "Epoch: 363, Loss Train: 3147937.7000, Accuracy: 0.8795, Specificity: 0.8492, Time: 35.056922\n",
            "Epoch: 364, Loss Train: 3145752.5995, Accuracy: 0.8794, Specificity: 0.8410, Time: 34.825930\n",
            "Epoch: 365, Loss Train: 3143567.8598, Accuracy: 0.8800, Specificity: 0.8470, Time: 34.746583\n",
            "Epoch: 366, Loss Train: 3141383.6689, Accuracy: 0.8797, Specificity: 0.8415, Time: 34.812691\n",
            "Epoch: 367, Loss Train: 3139200.2450, Accuracy: 0.8804, Specificity: 0.8528, Time: 35.223698\n",
            "Epoch: 368, Loss Train: 3137018.2208, Accuracy: 0.8798, Specificity: 0.8440, Time: 34.976719\n",
            "Epoch: 369, Loss Train: 3134838.6178, Accuracy: 0.8810, Specificity: 0.8430, Time: 35.148282\n",
            "Epoch: 370, Loss Train: 3132663.9443, Accuracy: 0.8795, Specificity: 0.8415, Time: 35.035608\n",
            "Epoch: 371, Loss Train: 3130498.9919, Accuracy: 0.8818, Specificity: 0.8415, Time: 35.020228\n",
            "Epoch: 372, Loss Train: 3128355.0368, Accuracy: 0.8787, Specificity: 0.8373, Time: 34.928205\n",
            "Epoch: 373, Loss Train: 3126255.5803, Accuracy: 0.8835, Specificity: 0.8462, Time: 35.061545\n",
            "Epoch: 374, Loss Train: 3124255.7809, Accuracy: 0.8765, Specificity: 0.8505, Time: 34.998932\n",
            "Epoch: 375, Loss Train: 3122472.7491, Accuracy: 0.8867, Specificity: 0.8333, Time: 35.056527\n",
            "Epoch: 376, Loss Train: 3121173.7781, Accuracy: 0.8721, Specificity: 0.8548, Time: 35.171835\n",
            "Epoch: 377, Loss Train: 3120840.1001, Accuracy: 0.8928, Specificity: 0.8293, Time: 35.065275\n",
            "Epoch: 378, Loss Train: 3122258.6493, Accuracy: 0.8645, Specificity: 0.8720, Time: 34.856400\n",
            "Epoch: 379, Loss Train: 3125195.2714, Accuracy: 0.8979, Specificity: 0.8210, Time: 35.086410\n",
            "Epoch: 380, Loss Train: 3126779.7614, Accuracy: 0.8653, Specificity: 0.8580, Time: 34.766381\n",
            "Epoch: 381, Loss Train: 3120282.2843, Accuracy: 0.8883, Specificity: 0.8355, Time: 35.247860\n",
            "Epoch: 382, Loss Train: 3109520.0603, Accuracy: 0.8851, Specificity: 0.8450, Time: 35.124697\n",
            "Epoch: 383, Loss Train: 3105743.4422, Accuracy: 0.8698, Specificity: 0.8518, Time: 34.941845\n",
            "Epoch: 384, Loss Train: 3109446.8046, Accuracy: 0.8940, Specificity: 0.8382, Time: 34.811353\n",
            "Epoch: 385, Loss Train: 3109334.8717, Accuracy: 0.8744, Specificity: 0.8658, Time: 35.063482\n",
            "Epoch: 386, Loss Train: 3101804.6844, Accuracy: 0.8796, Specificity: 0.8505, Time: 35.138625\n",
            "Epoch: 387, Loss Train: 3097658.8608, Accuracy: 0.8908, Specificity: 0.8292, Time: 34.938287\n",
            "Epoch: 388, Loss Train: 3099691.7842, Accuracy: 0.8719, Specificity: 0.8548, Time: 35.134414\n",
            "Epoch: 389, Loss Train: 3098450.9438, Accuracy: 0.8866, Specificity: 0.8498, Time: 35.060981\n",
            "Epoch: 390, Loss Train: 3092680.7958, Accuracy: 0.8859, Specificity: 0.8527, Time: 34.921954\n",
            "Epoch: 391, Loss Train: 3090437.0588, Accuracy: 0.8739, Specificity: 0.8547, Time: 35.025748\n",
            "Epoch: 392, Loss Train: 3091323.0423, Accuracy: 0.8894, Specificity: 0.8342, Time: 34.912595\n",
            "Epoch: 393, Loss Train: 3088662.2551, Accuracy: 0.8811, Specificity: 0.8545, Time: 35.226538\n",
            "Epoch: 394, Loss Train: 3084335.6849, Accuracy: 0.8778, Specificity: 0.8500, Time: 35.254176\n",
            "Epoch: 395, Loss Train: 3083500.5357, Accuracy: 0.8895, Specificity: 0.8432, Time: 35.006387\n",
            "Epoch: 396, Loss Train: 3083027.0154, Accuracy: 0.8784, Specificity: 0.8558, Time: 34.927388\n",
            "Epoch: 397, Loss Train: 3079598.8427, Accuracy: 0.8818, Specificity: 0.8490, Time: 35.286494\n",
            "Epoch: 398, Loss Train: 3076867.8709, Accuracy: 0.8880, Specificity: 0.8455, Time: 35.078620\n",
            "Epoch: 399, Loss Train: 3076334.9320, Accuracy: 0.8776, Specificity: 0.8547, Time: 34.968046\n",
            "Epoch: 400, Loss Train: 3074555.7175, Accuracy: 0.8849, Specificity: 0.8420, Time: 35.295319\n",
            "Epoch: 401, Loss Train: 3071433.3106, Accuracy: 0.8857, Specificity: 0.8492, Time: 35.092514\n",
            "Epoch: 402, Loss Train: 3069765.7581, Accuracy: 0.8783, Specificity: 0.8468, Time: 35.225882\n",
            "Epoch: 403, Loss Train: 3068756.3109, Accuracy: 0.8868, Specificity: 0.8572, Time: 35.214426\n",
            "Epoch: 404, Loss Train: 3066385.5121, Accuracy: 0.8833, Specificity: 0.8528, Time: 35.219458\n",
            "Epoch: 405, Loss Train: 3063908.2090, Accuracy: 0.8803, Specificity: 0.8600, Time: 35.323921\n",
            "Epoch: 406, Loss Train: 3062594.6069, Accuracy: 0.8874, Specificity: 0.8460, Time: 35.017935\n",
            "Epoch: 407, Loss Train: 3061034.6958, Accuracy: 0.8816, Specificity: 0.8560, Time: 35.158017\n",
            "Epoch: 408, Loss Train: 3058607.9962, Accuracy: 0.8826, Specificity: 0.8570, Time: 35.363309\n",
            "Epoch: 409, Loss Train: 3056642.9058, Accuracy: 0.8868, Specificity: 0.8450, Time: 34.729092\n",
            "Epoch: 410, Loss Train: 3055261.7260, Accuracy: 0.8808, Specificity: 0.8480, Time: 35.288815\n",
            "Epoch: 411, Loss Train: 3053362.4549, Accuracy: 0.8847, Specificity: 0.8550, Time: 34.192454\n",
            "Epoch: 412, Loss Train: 3051132.9163, Accuracy: 0.8855, Specificity: 0.8552, Time: 35.142890\n",
            "Epoch: 413, Loss Train: 3049406.8949, Accuracy: 0.8812, Specificity: 0.8585, Time: 35.250988\n",
            "Epoch: 414, Loss Train: 3047842.9465, Accuracy: 0.8861, Specificity: 0.8432, Time: 34.841249\n",
            "Epoch: 415, Loss Train: 3045845.8658, Accuracy: 0.8840, Specificity: 0.8583, Time: 35.108372\n",
            "Epoch: 416, Loss Train: 3043808.6455, Accuracy: 0.8824, Specificity: 0.8545, Time: 34.717025\n",
            "Epoch: 417, Loss Train: 3042137.2352, Accuracy: 0.8866, Specificity: 0.8488, Time: 34.801959\n",
            "Epoch: 418, Loss Train: 3040439.7994, Accuracy: 0.8830, Specificity: 0.8560, Time: 34.989908\n",
            "Epoch: 419, Loss Train: 3038454.4008, Accuracy: 0.8843, Specificity: 0.8550, Time: 35.027742\n",
            "Epoch: 420, Loss Train: 3036538.0541, Accuracy: 0.8862, Specificity: 0.8505, Time: 35.213264\n",
            "Epoch: 421, Loss Train: 3034848.5740, Accuracy: 0.8827, Specificity: 0.8448, Time: 35.181571\n",
            "Epoch: 422, Loss Train: 3033081.1388, Accuracy: 0.8859, Specificity: 0.8492, Time: 34.889512\n",
            "Epoch: 423, Loss Train: 3031143.1245, Accuracy: 0.8853, Specificity: 0.8570, Time: 35.052562\n",
            "Epoch: 424, Loss Train: 3029284.9904, Accuracy: 0.8835, Specificity: 0.8548, Time: 35.102278\n",
            "Epoch: 425, Loss Train: 3027562.9730, Accuracy: 0.8866, Specificity: 0.8330, Time: 34.981153\n",
            "Epoch: 426, Loss Train: 3025772.8460, Accuracy: 0.8845, Specificity: 0.8568, Time: 34.478791\n",
            "Epoch: 427, Loss Train: 3023877.6542, Accuracy: 0.8850, Specificity: 0.8600, Time: 34.864165\n",
            "Epoch: 428, Loss Train: 3022042.7529, Accuracy: 0.8864, Specificity: 0.8540, Time: 35.218559\n",
            "Epoch: 429, Loss Train: 3020294.7149, Accuracy: 0.8842, Specificity: 0.8520, Time: 35.074040\n",
            "Epoch: 430, Loss Train: 3018501.8793, Accuracy: 0.8862, Specificity: 0.8568, Time: 34.789501\n",
            "Epoch: 431, Loss Train: 3016640.4803, Accuracy: 0.8858, Specificity: 0.8600, Time: 35.179977\n",
            "Epoch: 432, Loss Train: 3014813.5740, Accuracy: 0.8850, Specificity: 0.8522, Time: 35.190381\n",
            "Epoch: 433, Loss Train: 3013046.6794, Accuracy: 0.8867, Specificity: 0.8540, Time: 35.131043\n",
            "Epoch: 434, Loss Train: 3011259.4146, Accuracy: 0.8853, Specificity: 0.8542, Time: 35.138480\n",
            "Epoch: 435, Loss Train: 3009423.5187, Accuracy: 0.8860, Specificity: 0.8520, Time: 35.133351\n",
            "Epoch: 436, Loss Train: 3007600.3758, Accuracy: 0.8867, Specificity: 0.8490, Time: 35.159242\n",
            "Epoch: 437, Loss Train: 3005819.8218, Accuracy: 0.8854, Specificity: 0.8517, Time: 34.967939\n",
            "Epoch: 438, Loss Train: 3004037.6308, Accuracy: 0.8867, Specificity: 0.8473, Time: 35.069652\n",
            "Epoch: 439, Loss Train: 3002222.9487, Accuracy: 0.8861, Specificity: 0.8425, Time: 34.980192\n",
            "Epoch: 440, Loss Train: 3000404.7086, Accuracy: 0.8860, Specificity: 0.8470, Time: 35.063623\n",
            "Epoch: 441, Loss Train: 2998613.6746, Accuracy: 0.8871, Specificity: 0.8498, Time: 34.618872\n",
            "Epoch: 442, Loss Train: 2996833.4839, Accuracy: 0.8858, Specificity: 0.8490, Time: 35.073818\n",
            "Epoch: 443, Loss Train: 2995035.7215, Accuracy: 0.8869, Specificity: 0.8572, Time: 35.030896\n",
            "Epoch: 444, Loss Train: 2993226.2563, Accuracy: 0.8869, Specificity: 0.8538, Time: 34.902664\n",
            "Epoch: 445, Loss Train: 2991428.8394, Accuracy: 0.8863, Specificity: 0.8518, Time: 35.005591\n",
            "Epoch: 446, Loss Train: 2989646.1352, Accuracy: 0.8874, Specificity: 0.8518, Time: 35.113379\n",
            "Epoch: 447, Loss Train: 2987860.1775, Accuracy: 0.8864, Specificity: 0.8575, Time: 35.153939\n",
            "Epoch: 448, Loss Train: 2986062.6247, Accuracy: 0.8869, Specificity: 0.8525, Time: 35.037274\n",
            "Epoch: 449, Loss Train: 2984264.9668, Accuracy: 0.8872, Specificity: 0.8490, Time: 35.193552\n",
            "Epoch: 450, Loss Train: 2982477.8274, Accuracy: 0.8866, Specificity: 0.8462, Time: 35.148935\n",
            "Epoch: 451, Loss Train: 2980696.4825, Accuracy: 0.8875, Specificity: 0.8682, Time: 35.140559\n",
            "Epoch: 452, Loss Train: 2978910.6404, Accuracy: 0.8869, Specificity: 0.8545, Time: 34.954576\n",
            "Epoch: 453, Loss Train: 2977119.3530, Accuracy: 0.8874, Specificity: 0.8477, Time: 35.238276\n",
            "Epoch: 454, Loss Train: 2975330.5158, Accuracy: 0.8877, Specificity: 0.8540, Time: 35.162543\n",
            "Epoch: 455, Loss Train: 2973548.4295, Accuracy: 0.8871, Specificity: 0.8488, Time: 35.180747\n",
            "Epoch: 456, Loss Train: 2971769.1131, Accuracy: 0.8880, Specificity: 0.8652, Time: 35.009195\n",
            "Epoch: 457, Loss Train: 2969987.1844, Accuracy: 0.8875, Specificity: 0.8565, Time: 34.985400\n",
            "Epoch: 458, Loss Train: 2968202.7303, Accuracy: 0.8877, Specificity: 0.8478, Time: 34.988598\n",
            "Epoch: 459, Loss Train: 2966420.2031, Accuracy: 0.8880, Specificity: 0.8605, Time: 35.237206\n",
            "Epoch: 460, Loss Train: 2964641.9441, Accuracy: 0.8877, Specificity: 0.8567, Time: 34.925399\n",
            "Epoch: 461, Loss Train: 2962865.9532, Accuracy: 0.8882, Specificity: 0.8538, Time: 34.985450\n",
            "Epoch: 462, Loss Train: 2961089.1981, Accuracy: 0.8878, Specificity: 0.8513, Time: 35.238371\n",
            "Epoch: 463, Loss Train: 2959311.1896, Accuracy: 0.8882, Specificity: 0.8582, Time: 35.054365\n",
            "Epoch: 464, Loss Train: 2957534.1230, Accuracy: 0.8882, Specificity: 0.8590, Time: 35.145964\n",
            "Epoch: 465, Loss Train: 2955759.7324, Accuracy: 0.8881, Specificity: 0.8535, Time: 35.012568\n",
            "Epoch: 466, Loss Train: 2953987.6199, Accuracy: 0.8886, Specificity: 0.8648, Time: 35.089341\n",
            "Epoch: 467, Loss Train: 2952216.1555, Accuracy: 0.8882, Specificity: 0.8630, Time: 35.252383\n",
            "Epoch: 468, Loss Train: 2950444.3647, Accuracy: 0.8886, Specificity: 0.8587, Time: 34.933303\n",
            "Epoch: 469, Loss Train: 2948672.8836, Accuracy: 0.8884, Specificity: 0.8585, Time: 35.167007\n",
            "Epoch: 470, Loss Train: 2946902.8493, Accuracy: 0.8885, Specificity: 0.8607, Time: 35.197424\n",
            "Epoch: 471, Loss Train: 2945134.7870, Accuracy: 0.8888, Specificity: 0.8510, Time: 35.202929\n",
            "Epoch: 472, Loss Train: 2943368.2176, Accuracy: 0.8885, Specificity: 0.8545, Time: 35.216568\n",
            "Epoch: 473, Loss Train: 2941602.3212, Accuracy: 0.8889, Specificity: 0.8568, Time: 35.244061\n",
            "Epoch: 474, Loss Train: 2939836.8233, Accuracy: 0.8887, Specificity: 0.8558, Time: 35.027142\n",
            "Epoch: 475, Loss Train: 2938072.0168, Accuracy: 0.8888, Specificity: 0.8658, Time: 35.286527\n",
            "Epoch: 476, Loss Train: 2936308.4745, Accuracy: 0.8889, Specificity: 0.8650, Time: 35.075519\n",
            "Epoch: 477, Loss Train: 2934546.4507, Accuracy: 0.8887, Specificity: 0.8560, Time: 34.949991\n",
            "Epoch: 478, Loss Train: 2932785.7890, Accuracy: 0.8891, Specificity: 0.8547, Time: 35.003330\n",
            "Epoch: 479, Loss Train: 2931026.1517, Accuracy: 0.8888, Specificity: 0.8640, Time: 34.853194\n",
            "Epoch: 480, Loss Train: 2929267.2898, Accuracy: 0.8891, Specificity: 0.8558, Time: 35.263988\n",
            "Epoch: 481, Loss Train: 2927509.2711, Accuracy: 0.8891, Specificity: 0.8565, Time: 34.982122\n",
            "Epoch: 482, Loss Train: 2925752.2892, Accuracy: 0.8892, Specificity: 0.8482, Time: 34.955472\n",
            "Epoch: 483, Loss Train: 2923996.5602, Accuracy: 0.8894, Specificity: 0.8615, Time: 35.184667\n",
            "Epoch: 484, Loss Train: 2922242.1490, Accuracy: 0.8893, Specificity: 0.8682, Time: 35.152361\n",
            "Epoch: 485, Loss Train: 2920488.9739, Accuracy: 0.8895, Specificity: 0.8528, Time: 35.184114\n",
            "Epoch: 486, Loss Train: 2918736.9203, Accuracy: 0.8894, Specificity: 0.8648, Time: 35.226207\n",
            "Epoch: 487, Loss Train: 2916985.8818, Accuracy: 0.8896, Specificity: 0.8540, Time: 34.869269\n",
            "Epoch: 488, Loss Train: 2915235.8803, Accuracy: 0.8895, Specificity: 0.8620, Time: 35.272100\n",
            "Epoch: 489, Loss Train: 2913486.9605, Accuracy: 0.8896, Specificity: 0.8578, Time: 35.402935\n",
            "Epoch: 490, Loss Train: 2911739.2214, Accuracy: 0.8897, Specificity: 0.8575, Time: 34.980393\n",
            "Epoch: 491, Loss Train: 2909992.7166, Accuracy: 0.8897, Specificity: 0.8578, Time: 34.863131\n",
            "Epoch: 492, Loss Train: 2908247.4679, Accuracy: 0.8898, Specificity: 0.8615, Time: 35.179925\n",
            "Epoch: 493, Loss Train: 2906503.4532, Accuracy: 0.8898, Specificity: 0.8568, Time: 35.163866\n",
            "Epoch: 494, Loss Train: 2904760.6423, Accuracy: 0.8899, Specificity: 0.8545, Time: 35.105153\n",
            "Epoch: 495, Loss Train: 2903019.0225, Accuracy: 0.8898, Specificity: 0.8562, Time: 34.141910\n",
            "Epoch: 496, Loss Train: 2901278.5702, Accuracy: 0.8900, Specificity: 0.8565, Time: 35.388785\n",
            "Epoch: 497, Loss Train: 2899539.3110, Accuracy: 0.8899, Specificity: 0.8477, Time: 35.165003\n",
            "Epoch: 498, Loss Train: 2897801.2542, Accuracy: 0.8901, Specificity: 0.8533, Time: 34.932555\n",
            "Epoch: 499, Loss Train: 2896064.4377, Accuracy: 0.8901, Specificity: 0.8518, Time: 35.127444\n",
            "Epoch: 500, Loss Train: 2894328.8709, Accuracy: 0.8901, Specificity: 0.8562, Time: 35.123683\n",
            "Epoch: 501, Loss Train: 2892594.5769, Accuracy: 0.8901, Specificity: 0.8542, Time: 35.096233\n",
            "Epoch: 502, Loss Train: 2890861.5626, Accuracy: 0.8902, Specificity: 0.8608, Time: 35.181764\n",
            "Epoch: 503, Loss Train: 2889129.8367, Accuracy: 0.8903, Specificity: 0.8495, Time: 35.070397\n",
            "Epoch: 504, Loss Train: 2887399.4038, Accuracy: 0.8903, Specificity: 0.8592, Time: 35.030862\n",
            "Epoch: 505, Loss Train: 2885670.2681, Accuracy: 0.8903, Specificity: 0.8580, Time: 35.126827\n",
            "Epoch: 506, Loss Train: 2883942.4321, Accuracy: 0.8903, Specificity: 0.8580, Time: 35.171129\n",
            "Epoch: 507, Loss Train: 2882215.9027, Accuracy: 0.8904, Specificity: 0.8572, Time: 35.164088\n",
            "Epoch: 508, Loss Train: 2880490.6903, Accuracy: 0.8904, Specificity: 0.8573, Time: 35.087526\n",
            "Epoch: 509, Loss Train: 2878766.7969, Accuracy: 0.8905, Specificity: 0.8625, Time: 35.095174\n",
            "Epoch: 510, Loss Train: 2877044.2444, Accuracy: 0.8905, Specificity: 0.8538, Time: 35.034060\n",
            "Epoch: 511, Loss Train: 2875323.0268, Accuracy: 0.8907, Specificity: 0.8628, Time: 35.248740\n",
            "Epoch: 512, Loss Train: 2873603.1732, Accuracy: 0.8907, Specificity: 0.8600, Time: 35.409537\n",
            "Epoch: 513, Loss Train: 2871884.6797, Accuracy: 0.8910, Specificity: 0.8610, Time: 35.149930\n",
            "Epoch: 514, Loss Train: 2870167.5868, Accuracy: 0.8908, Specificity: 0.8647, Time: 35.107147\n",
            "Epoch: 515, Loss Train: 2868451.8892, Accuracy: 0.8911, Specificity: 0.8667, Time: 35.172529\n",
            "Epoch: 516, Loss Train: 2866737.6478, Accuracy: 0.8910, Specificity: 0.8572, Time: 35.231659\n",
            "Epoch: 517, Loss Train: 2865024.8796, Accuracy: 0.8914, Specificity: 0.8572, Time: 35.276639\n",
            "Epoch: 518, Loss Train: 2863313.7005, Accuracy: 0.8910, Specificity: 0.8520, Time: 35.295528\n",
            "Epoch: 519, Loss Train: 2861604.1996, Accuracy: 0.8916, Specificity: 0.8547, Time: 35.521737\n",
            "Epoch: 520, Loss Train: 2859896.6520, Accuracy: 0.8910, Specificity: 0.8575, Time: 35.134438\n",
            "Epoch: 521, Loss Train: 2858191.4012, Accuracy: 0.8919, Specificity: 0.8610, Time: 35.241169\n",
            "Epoch: 522, Loss Train: 2856489.2650, Accuracy: 0.8909, Specificity: 0.8555, Time: 35.253181\n",
            "Epoch: 523, Loss Train: 2854791.5098, Accuracy: 0.8923, Specificity: 0.8638, Time: 35.412948\n",
            "Epoch: 524, Loss Train: 2853100.8916, Accuracy: 0.8905, Specificity: 0.8595, Time: 35.594841\n",
            "Epoch: 525, Loss Train: 2851422.2599, Accuracy: 0.8929, Specificity: 0.8548, Time: 35.076491\n",
            "Epoch: 526, Loss Train: 2849765.8514, Accuracy: 0.8900, Specificity: 0.8682, Time: 35.232538\n",
            "Epoch: 527, Loss Train: 2848150.8263, Accuracy: 0.8941, Specificity: 0.8612, Time: 35.285187\n",
            "Epoch: 528, Loss Train: 2846617.6305, Accuracy: 0.8883, Specificity: 0.8682, Time: 35.147833\n",
            "Epoch: 529, Loss Train: 2845242.6572, Accuracy: 0.8964, Specificity: 0.8618, Time: 35.342953\n",
            "Epoch: 530, Loss Train: 2844184.3916, Accuracy: 0.8852, Specificity: 0.8648, Time: 35.201088\n",
            "Epoch: 531, Loss Train: 2843709.8222, Accuracy: 0.9008, Specificity: 0.8508, Time: 35.143395\n",
            "Epoch: 532, Loss Train: 2844285.3841, Accuracy: 0.8795, Specificity: 0.8740, Time: 35.331826\n",
            "Epoch: 533, Loss Train: 2846193.7863, Accuracy: 0.9061, Specificity: 0.8345, Time: 35.041454\n",
            "Epoch: 534, Loss Train: 2849043.8886, Accuracy: 0.8751, Specificity: 0.8782, Time: 35.089012\n",
            "Epoch: 535, Loss Train: 2849392.5238, Accuracy: 0.9051, Specificity: 0.8492, Time: 35.174080\n",
            "Epoch: 536, Loss Train: 2844107.3719, Accuracy: 0.8844, Specificity: 0.8730, Time: 35.183907\n",
            "Epoch: 537, Loss Train: 2834812.0435, Accuracy: 0.8915, Specificity: 0.8570, Time: 35.138911\n",
            "Epoch: 538, Loss Train: 2830060.6990, Accuracy: 0.9001, Specificity: 0.8498, Time: 35.271833\n",
            "Epoch: 539, Loss Train: 2832175.3379, Accuracy: 0.8805, Specificity: 0.8747, Time: 35.159450\n",
            "Epoch: 540, Loss Train: 2834426.1199, Accuracy: 0.9018, Specificity: 0.8522, Time: 35.304166\n",
            "Epoch: 541, Loss Train: 2831002.6875, Accuracy: 0.8884, Specificity: 0.8715, Time: 35.058163\n",
            "Epoch: 542, Loss Train: 2824852.1528, Accuracy: 0.8893, Specificity: 0.8522, Time: 35.006299\n",
            "Epoch: 543, Loss Train: 2823056.3682, Accuracy: 0.9007, Specificity: 0.8542, Time: 35.179518\n",
            "Epoch: 544, Loss Train: 2824770.8305, Accuracy: 0.8836, Specificity: 0.8668, Time: 34.970501\n",
            "Epoch: 545, Loss Train: 2823764.0858, Accuracy: 0.8974, Specificity: 0.8585, Time: 35.126538\n",
            "Epoch: 546, Loss Train: 2819362.5699, Accuracy: 0.8936, Specificity: 0.8660, Time: 34.669767\n",
            "Epoch: 547, Loss Train: 2816607.4384, Accuracy: 0.8867, Specificity: 0.8622, Time: 35.125494\n",
            "Epoch: 548, Loss Train: 2817018.4591, Accuracy: 0.9000, Specificity: 0.8585, Time: 35.297493\n",
            "Epoch: 549, Loss Train: 2816554.3280, Accuracy: 0.8881, Specificity: 0.8542, Time: 35.179538\n",
            "Epoch: 550, Loss Train: 2813366.8362, Accuracy: 0.8925, Specificity: 0.8620, Time: 35.401963\n",
            "Epoch: 551, Loss Train: 2810690.5469, Accuracy: 0.8971, Specificity: 0.8520, Time: 35.451203\n",
            "Epoch: 552, Loss Train: 2810295.2684, Accuracy: 0.8869, Specificity: 0.8718, Time: 35.239837\n",
            "Epoch: 553, Loss Train: 2809716.5613, Accuracy: 0.8970, Specificity: 0.8558, Time: 35.217377\n",
            "Epoch: 554, Loss Train: 2807307.9695, Accuracy: 0.8925, Specificity: 0.8622, Time: 35.325333\n",
            "Epoch: 555, Loss Train: 2804895.6636, Accuracy: 0.8900, Specificity: 0.8578, Time: 35.188415\n",
            "Epoch: 556, Loss Train: 2804014.9140, Accuracy: 0.8979, Specificity: 0.8580, Time: 35.123339\n",
            "Epoch: 557, Loss Train: 2803276.3747, Accuracy: 0.8896, Specificity: 0.8625, Time: 35.135226\n",
            "Epoch: 558, Loss Train: 2801311.7320, Accuracy: 0.8938, Specificity: 0.8650, Time: 35.282348\n",
            "Epoch: 559, Loss Train: 2799152.8324, Accuracy: 0.8954, Specificity: 0.8532, Time: 35.185986\n",
            "Epoch: 560, Loss Train: 2797976.4679, Accuracy: 0.8896, Specificity: 0.8797, Time: 34.873574\n",
            "Epoch: 561, Loss Train: 2797066.3125, Accuracy: 0.8964, Specificity: 0.8605, Time: 35.013824\n",
            "Epoch: 562, Loss Train: 2795405.1900, Accuracy: 0.8924, Specificity: 0.8620, Time: 35.324090\n",
            "Epoch: 563, Loss Train: 2793449.2090, Accuracy: 0.8921, Specificity: 0.8688, Time: 34.999518\n",
            "Epoch: 564, Loss Train: 2792083.4730, Accuracy: 0.8964, Specificity: 0.8600, Time: 35.374685\n",
            "Epoch: 565, Loss Train: 2791035.7920, Accuracy: 0.8908, Specificity: 0.8580, Time: 35.088446\n",
            "Epoch: 566, Loss Train: 2789559.9382, Accuracy: 0.8948, Specificity: 0.8677, Time: 35.162954\n",
            "Epoch: 567, Loss Train: 2787776.6764, Accuracy: 0.8943, Specificity: 0.8683, Time: 35.372708\n",
            "Epoch: 568, Loss Train: 2786295.6719, Accuracy: 0.8917, Specificity: 0.8685, Time: 35.128689\n",
            "Epoch: 569, Loss Train: 2785120.6644, Accuracy: 0.8961, Specificity: 0.8655, Time: 35.408945\n",
            "Epoch: 570, Loss Train: 2783762.6697, Accuracy: 0.8924, Specificity: 0.8588, Time: 35.402878\n",
            "Epoch: 571, Loss Train: 2782126.6079, Accuracy: 0.8937, Specificity: 0.8610, Time: 35.186779\n",
            "Epoch: 572, Loss Train: 2780590.1453, Accuracy: 0.8954, Specificity: 0.8653, Time: 35.336444\n",
            "Epoch: 573, Loss Train: 2779302.1876, Accuracy: 0.8920, Specificity: 0.8545, Time: 35.093238\n",
            "Epoch: 574, Loss Train: 2777994.0804, Accuracy: 0.8956, Specificity: 0.8538, Time: 35.299957\n",
            "Epoch: 575, Loss Train: 2776486.4255, Accuracy: 0.8937, Specificity: 0.8592, Time: 35.364046\n",
            "Epoch: 576, Loss Train: 2774947.5864, Accuracy: 0.8933, Specificity: 0.8645, Time: 35.386894\n",
            "Epoch: 577, Loss Train: 2773565.9100, Accuracy: 0.8958, Specificity: 0.8605, Time: 35.383194\n",
            "Epoch: 578, Loss Train: 2772256.1040, Accuracy: 0.8928, Specificity: 0.8698, Time: 35.184251\n",
            "Epoch: 579, Loss Train: 2770843.7522, Accuracy: 0.8951, Specificity: 0.8625, Time: 35.249342\n",
            "Epoch: 580, Loss Train: 2769346.0985, Accuracy: 0.8945, Specificity: 0.8675, Time: 35.242723\n",
            "Epoch: 581, Loss Train: 2767905.7984, Accuracy: 0.8935, Specificity: 0.8645, Time: 35.340538\n",
            "Epoch: 582, Loss Train: 2766556.4623, Accuracy: 0.8958, Specificity: 0.8598, Time: 35.176994\n",
            "Epoch: 583, Loss Train: 2765195.6707, Accuracy: 0.8936, Specificity: 0.8687, Time: 35.311123\n",
            "Epoch: 584, Loss Train: 2763759.8344, Accuracy: 0.8951, Specificity: 0.8590, Time: 35.501564\n",
            "Epoch: 585, Loss Train: 2762307.6574, Accuracy: 0.8952, Specificity: 0.8645, Time: 35.552739\n",
            "Epoch: 586, Loss Train: 2760911.2161, Accuracy: 0.8938, Specificity: 0.8660, Time: 35.605728\n",
            "Epoch: 587, Loss Train: 2759553.1756, Accuracy: 0.8958, Specificity: 0.8650, Time: 35.354942\n",
            "Epoch: 588, Loss Train: 2758169.1692, Accuracy: 0.8941, Specificity: 0.8665, Time: 35.564982\n",
            "Epoch: 589, Loss Train: 2756744.7204, Accuracy: 0.8951, Specificity: 0.8630, Time: 35.570965\n",
            "Epoch: 590, Loss Train: 2755324.9381, Accuracy: 0.8955, Specificity: 0.8573, Time: 35.541317\n",
            "Epoch: 591, Loss Train: 2753941.5292, Accuracy: 0.8943, Specificity: 0.8755, Time: 35.760327\n",
            "Epoch: 592, Loss Train: 2752574.1489, Accuracy: 0.8957, Specificity: 0.8740, Time: 35.481863\n",
            "Epoch: 593, Loss Train: 2751187.7558, Accuracy: 0.8946, Specificity: 0.8578, Time: 35.355729\n",
            "Epoch: 594, Loss Train: 2749780.2251, Accuracy: 0.8952, Specificity: 0.8645, Time: 35.713779\n",
            "Epoch: 595, Loss Train: 2748378.5279, Accuracy: 0.8954, Specificity: 0.8607, Time: 35.466507\n",
            "Epoch: 596, Loss Train: 2746998.5411, Accuracy: 0.8947, Specificity: 0.8500, Time: 35.681592\n",
            "Epoch: 597, Loss Train: 2745627.9724, Accuracy: 0.8959, Specificity: 0.8655, Time: 35.831324\n",
            "Epoch: 598, Loss Train: 2744247.3277, Accuracy: 0.8949, Specificity: 0.8658, Time: 35.596201\n",
            "Epoch: 599, Loss Train: 2742854.5219, Accuracy: 0.8955, Specificity: 0.8628, Time: 35.505964\n",
            "Epoch: 600, Loss Train: 2741463.9211, Accuracy: 0.8956, Specificity: 0.8748, Time: 35.654252\n",
            "Epoch: 601, Loss Train: 2740085.8475, Accuracy: 0.8949, Specificity: 0.8630, Time: 35.749264\n",
            "Epoch: 602, Loss Train: 2738715.7293, Accuracy: 0.8959, Specificity: 0.8690, Time: 35.746824\n",
            "Epoch: 603, Loss Train: 2737342.4799, Accuracy: 0.8951, Specificity: 0.8712, Time: 35.757334\n",
            "Epoch: 604, Loss Train: 2735962.0606, Accuracy: 0.8957, Specificity: 0.8662, Time: 35.526119\n",
            "Epoch: 605, Loss Train: 2734580.5826, Accuracy: 0.8956, Specificity: 0.8685, Time: 35.122358\n",
            "Epoch: 606, Loss Train: 2733205.3129, Accuracy: 0.8954, Specificity: 0.8720, Time: 35.813496\n",
            "Epoch: 607, Loss Train: 2731836.8598, Accuracy: 0.8961, Specificity: 0.8592, Time: 35.638171\n",
            "Epoch: 608, Loss Train: 2730469.9612, Accuracy: 0.8954, Specificity: 0.8620, Time: 34.848049\n",
            "Epoch: 609, Loss Train: 2729100.0715, Accuracy: 0.8961, Specificity: 0.8707, Time: 35.727826\n",
            "Epoch: 610, Loss Train: 2727727.9130, Accuracy: 0.8957, Specificity: 0.8732, Time: 36.007500\n",
            "Epoch: 611, Loss Train: 2726357.4248, Accuracy: 0.8958, Specificity: 0.8650, Time: 35.756862\n",
            "Epoch: 612, Loss Train: 2724991.3685, Accuracy: 0.8961, Specificity: 0.8713, Time: 35.922697\n",
            "Epoch: 613, Loss Train: 2723628.9030, Accuracy: 0.8956, Specificity: 0.8640, Time: 35.967330\n",
            "Epoch: 614, Loss Train: 2722267.1462, Accuracy: 0.8962, Specificity: 0.8712, Time: 35.759097\n",
            "Epoch: 615, Loss Train: 2720904.3520, Accuracy: 0.8959, Specificity: 0.8690, Time: 35.858470\n",
            "Epoch: 616, Loss Train: 2719541.0881, Accuracy: 0.8963, Specificity: 0.8555, Time: 35.766439\n",
            "Epoch: 617, Loss Train: 2718179.3413, Accuracy: 0.8963, Specificity: 0.8658, Time: 35.787744\n",
            "Epoch: 618, Loss Train: 2716820.3888, Accuracy: 0.8961, Specificity: 0.8712, Time: 36.001158\n",
            "Epoch: 619, Loss Train: 2715463.9394, Accuracy: 0.8965, Specificity: 0.8712, Time: 35.681798\n",
            "Epoch: 620, Loss Train: 2714108.7311, Accuracy: 0.8961, Specificity: 0.8688, Time: 35.846673\n",
            "Epoch: 621, Loss Train: 2712753.7184, Accuracy: 0.8965, Specificity: 0.8670, Time: 35.914128\n",
            "Epoch: 622, Loss Train: 2711398.9195, Accuracy: 0.8963, Specificity: 0.8695, Time: 35.064284\n",
            "Epoch: 623, Loss Train: 2710045.0565, Accuracy: 0.8964, Specificity: 0.8682, Time: 35.886549\n",
            "Epoch: 624, Loss Train: 2708692.9387, Accuracy: 0.8965, Specificity: 0.8678, Time: 35.723927\n",
            "Epoch: 625, Loss Train: 2707342.7940, Accuracy: 0.8964, Specificity: 0.8645, Time: 35.706836\n",
            "Epoch: 626, Loss Train: 2705994.2700, Accuracy: 0.8967, Specificity: 0.8618, Time: 36.031298\n",
            "Epoch: 627, Loss Train: 2704646.8268, Accuracy: 0.8964, Specificity: 0.8795, Time: 35.735112\n",
            "Epoch: 628, Loss Train: 2703300.1009, Accuracy: 0.8968, Specificity: 0.8715, Time: 35.878882\n",
            "Epoch: 629, Loss Train: 2701954.1547, Accuracy: 0.8967, Specificity: 0.8755, Time: 35.937587\n",
            "Epoch: 630, Loss Train: 2700609.2647, Accuracy: 0.8968, Specificity: 0.8670, Time: 35.887724\n",
            "Epoch: 631, Loss Train: 2699265.7674, Accuracy: 0.8968, Specificity: 0.8678, Time: 36.021631\n",
            "Epoch: 632, Loss Train: 2697923.7912, Accuracy: 0.8968, Specificity: 0.8628, Time: 35.932028\n",
            "Epoch: 633, Loss Train: 2696583.2634, Accuracy: 0.8970, Specificity: 0.8695, Time: 35.878516\n",
            "Epoch: 634, Loss Train: 2695244.0026, Accuracy: 0.8969, Specificity: 0.8755, Time: 36.102896\n",
            "Epoch: 635, Loss Train: 2693905.8152, Accuracy: 0.8971, Specificity: 0.8642, Time: 35.320918\n",
            "Epoch: 636, Loss Train: 2692568.6406, Accuracy: 0.8971, Specificity: 0.8667, Time: 36.208256\n",
            "Epoch: 637, Loss Train: 2691232.4952, Accuracy: 0.8972, Specificity: 0.8708, Time: 35.979741\n",
            "Epoch: 638, Loss Train: 2689897.4975, Accuracy: 0.8972, Specificity: 0.8695, Time: 35.954227\n",
            "Epoch: 639, Loss Train: 2688563.7288, Accuracy: 0.8973, Specificity: 0.8762, Time: 35.975074\n",
            "Epoch: 640, Loss Train: 2687231.2666, Accuracy: 0.8973, Specificity: 0.8582, Time: 36.074434\n",
            "Epoch: 641, Loss Train: 2685900.1028, Accuracy: 0.8973, Specificity: 0.8675, Time: 36.112139\n",
            "Epoch: 642, Loss Train: 2684570.2013, Accuracy: 0.8974, Specificity: 0.8575, Time: 36.263077\n",
            "Epoch: 643, Loss Train: 2683241.5098, Accuracy: 0.8974, Specificity: 0.8748, Time: 36.025733\n",
            "Epoch: 644, Loss Train: 2681913.9812, Accuracy: 0.8975, Specificity: 0.8785, Time: 36.144800\n",
            "Epoch: 645, Loss Train: 2680587.5983, Accuracy: 0.8975, Specificity: 0.8725, Time: 36.216082\n",
            "Epoch: 646, Loss Train: 2679262.3437, Accuracy: 0.8976, Specificity: 0.8652, Time: 36.008159\n",
            "Epoch: 647, Loss Train: 2677938.2481, Accuracy: 0.8975, Specificity: 0.8660, Time: 36.195031\n",
            "Epoch: 648, Loss Train: 2676615.3181, Accuracy: 0.8977, Specificity: 0.8700, Time: 36.124580\n",
            "Epoch: 649, Loss Train: 2675293.5785, Accuracy: 0.8976, Specificity: 0.8680, Time: 36.051459\n",
            "Epoch: 650, Loss Train: 2673973.0365, Accuracy: 0.8978, Specificity: 0.8702, Time: 35.936874\n",
            "Epoch: 651, Loss Train: 2672653.7109, Accuracy: 0.8979, Specificity: 0.8642, Time: 35.892968\n",
            "Epoch: 652, Loss Train: 2671335.5860, Accuracy: 0.8980, Specificity: 0.8725, Time: 36.069166\n",
            "Epoch: 653, Loss Train: 2670018.6657, Accuracy: 0.8981, Specificity: 0.8745, Time: 36.022452\n",
            "Epoch: 654, Loss Train: 2668702.9452, Accuracy: 0.8981, Specificity: 0.8782, Time: 36.013957\n",
            "Epoch: 655, Loss Train: 2667388.4174, Accuracy: 0.8982, Specificity: 0.8658, Time: 36.013146\n",
            "Epoch: 656, Loss Train: 2666075.0753, Accuracy: 0.8982, Specificity: 0.8790, Time: 35.718490\n",
            "Epoch: 657, Loss Train: 2664762.9206, Accuracy: 0.8983, Specificity: 0.8650, Time: 36.124175\n",
            "Epoch: 658, Loss Train: 2663451.9497, Accuracy: 0.8982, Specificity: 0.8662, Time: 36.117531\n",
            "Epoch: 659, Loss Train: 2662142.1602, Accuracy: 0.8983, Specificity: 0.8798, Time: 35.961365\n",
            "Epoch: 660, Loss Train: 2660833.5594, Accuracy: 0.8983, Specificity: 0.8655, Time: 36.124933\n",
            "Epoch: 661, Loss Train: 2659526.1420, Accuracy: 0.8985, Specificity: 0.8807, Time: 35.976121\n",
            "Epoch: 662, Loss Train: 2658219.9221, Accuracy: 0.8984, Specificity: 0.8700, Time: 36.267358\n",
            "Epoch: 663, Loss Train: 2656914.8932, Accuracy: 0.8987, Specificity: 0.8740, Time: 36.199009\n",
            "Epoch: 664, Loss Train: 2655611.0847, Accuracy: 0.8986, Specificity: 0.8700, Time: 35.854991\n",
            "Epoch: 665, Loss Train: 2654308.4941, Accuracy: 0.8987, Specificity: 0.8775, Time: 35.924890\n",
            "Epoch: 666, Loss Train: 2653007.1611, Accuracy: 0.8986, Specificity: 0.8690, Time: 36.097097\n",
            "Epoch: 667, Loss Train: 2651707.1191, Accuracy: 0.8989, Specificity: 0.8670, Time: 36.080127\n",
            "Epoch: 668, Loss Train: 2650408.4565, Accuracy: 0.8985, Specificity: 0.8672, Time: 36.136033\n",
            "Epoch: 669, Loss Train: 2649111.2705, Accuracy: 0.8990, Specificity: 0.8710, Time: 35.823023\n",
            "Epoch: 670, Loss Train: 2647815.8048, Accuracy: 0.8985, Specificity: 0.8680, Time: 36.273697\n",
            "Epoch: 671, Loss Train: 2646522.3836, Accuracy: 0.8992, Specificity: 0.8675, Time: 36.138662\n",
            "Epoch: 672, Loss Train: 2645231.6972, Accuracy: 0.8984, Specificity: 0.8685, Time: 35.871705\n",
            "Epoch: 673, Loss Train: 2643944.8370, Accuracy: 0.8996, Specificity: 0.8613, Time: 36.069465\n",
            "Epoch: 674, Loss Train: 2642663.9883, Accuracy: 0.8982, Specificity: 0.8853, Time: 35.393975\n",
            "Epoch: 675, Loss Train: 2641392.8812, Accuracy: 0.9000, Specificity: 0.8718, Time: 36.024152\n",
            "Epoch: 676, Loss Train: 2640138.9506, Accuracy: 0.8977, Specificity: 0.8580, Time: 35.864294\n",
            "Epoch: 677, Loss Train: 2638915.5087, Accuracy: 0.9009, Specificity: 0.8690, Time: 35.836102\n",
            "Epoch: 678, Loss Train: 2637749.1902, Accuracy: 0.8962, Specificity: 0.8675, Time: 35.979066\n",
            "Epoch: 679, Loss Train: 2636688.3620, Accuracy: 0.9029, Specificity: 0.8767, Time: 35.831527\n",
            "Epoch: 680, Loss Train: 2635829.4302, Accuracy: 0.8938, Specificity: 0.8785, Time: 35.822251\n",
            "Epoch: 681, Loss Train: 2635337.0724, Accuracy: 0.9062, Specificity: 0.8610, Time: 35.962451\n",
            "Epoch: 682, Loss Train: 2635514.6358, Accuracy: 0.8894, Specificity: 0.8787, Time: 35.566497\n",
            "Epoch: 683, Loss Train: 2636712.9346, Accuracy: 0.9109, Specificity: 0.8590, Time: 35.857322\n",
            "Epoch: 684, Loss Train: 2639253.2559, Accuracy: 0.8839, Specificity: 0.8870, Time: 35.693783\n",
            "Epoch: 685, Loss Train: 2642118.7758, Accuracy: 0.9140, Specificity: 0.8562, Time: 34.912525\n",
            "Epoch: 686, Loss Train: 2642873.9399, Accuracy: 0.8850, Specificity: 0.8930, Time: 35.608903\n",
            "Epoch: 687, Loss Train: 2637681.8303, Accuracy: 0.9073, Specificity: 0.8705, Time: 35.847222\n",
            "Epoch: 688, Loss Train: 2629070.5499, Accuracy: 0.8992, Specificity: 0.8682, Time: 35.648107\n",
            "Epoch: 689, Loss Train: 2623888.9627, Accuracy: 0.8927, Specificity: 0.8820, Time: 35.852224\n",
            "Epoch: 690, Loss Train: 2625335.2228, Accuracy: 0.9097, Specificity: 0.8585, Time: 35.791674\n",
            "Epoch: 691, Loss Train: 2628569.3230, Accuracy: 0.8889, Specificity: 0.8895, Time: 35.805480\n",
            "Epoch: 692, Loss Train: 2627187.3175, Accuracy: 0.9061, Specificity: 0.8600, Time: 35.945493\n",
            "Epoch: 693, Loss Train: 2621709.0726, Accuracy: 0.8999, Specificity: 0.8747, Time: 35.894249\n",
            "Epoch: 694, Loss Train: 2618098.3236, Accuracy: 0.8938, Specificity: 0.8875, Time: 36.073833\n",
            "Epoch: 695, Loss Train: 2619017.3327, Accuracy: 0.9082, Specificity: 0.8600, Time: 35.846053\n",
            "Epoch: 696, Loss Train: 2620329.9742, Accuracy: 0.8923, Specificity: 0.8775, Time: 35.901791\n",
            "Epoch: 697, Loss Train: 2618000.5583, Accuracy: 0.9028, Specificity: 0.8682, Time: 36.252681\n",
            "Epoch: 698, Loss Train: 2614122.3367, Accuracy: 0.9021, Specificity: 0.8707, Time: 35.811278\n",
            "Epoch: 699, Loss Train: 2612746.6605, Accuracy: 0.8938, Specificity: 0.8742, Time: 35.780959\n",
            "Epoch: 700, Loss Train: 2613503.9438, Accuracy: 0.9066, Specificity: 0.8640, Time: 36.072347\n",
            "Epoch: 701, Loss Train: 2612917.2787, Accuracy: 0.8959, Specificity: 0.8733, Time: 35.523828\n",
            "Epoch: 702, Loss Train: 2610160.6295, Accuracy: 0.9003, Specificity: 0.8693, Time: 35.916247\n",
            "Epoch: 703, Loss Train: 2607965.1377, Accuracy: 0.9038, Specificity: 0.8628, Time: 35.826932\n",
            "Epoch: 704, Loss Train: 2607697.2837, Accuracy: 0.8948, Specificity: 0.8790, Time: 35.927844\n",
            "Epoch: 705, Loss Train: 2607519.9385, Accuracy: 0.9047, Specificity: 0.8735, Time: 35.871283\n",
            "Epoch: 706, Loss Train: 2605835.1661, Accuracy: 0.8989, Specificity: 0.8785, Time: 36.022032\n",
            "Epoch: 707, Loss Train: 2603663.1071, Accuracy: 0.8987, Specificity: 0.8780, Time: 36.126360\n",
            "Epoch: 708, Loss Train: 2602623.9832, Accuracy: 0.9044, Specificity: 0.8753, Time: 35.797672\n",
            "Epoch: 709, Loss Train: 2602292.7901, Accuracy: 0.8964, Specificity: 0.8820, Time: 35.948217\n",
            "Epoch: 710, Loss Train: 2601246.4348, Accuracy: 0.9028, Specificity: 0.8815, Time: 35.807425\n",
            "Epoch: 711, Loss Train: 2599447.2084, Accuracy: 0.9008, Specificity: 0.8638, Time: 35.887753\n",
            "Epoch: 712, Loss Train: 2598016.8142, Accuracy: 0.8981, Specificity: 0.8713, Time: 35.760003\n",
            "Epoch: 713, Loss Train: 2597318.0293, Accuracy: 0.9040, Specificity: 0.8680, Time: 36.083651\n",
            "Epoch: 714, Loss Train: 2596545.8925, Accuracy: 0.8981, Specificity: 0.8802, Time: 35.753397\n",
            "Epoch: 715, Loss Train: 2595163.3746, Accuracy: 0.9014, Specificity: 0.8725, Time: 35.839941\n",
            "Epoch: 716, Loss Train: 2593664.0903, Accuracy: 0.9020, Specificity: 0.8825, Time: 35.910678\n",
            "Epoch: 717, Loss Train: 2592623.3345, Accuracy: 0.8984, Specificity: 0.8732, Time: 35.092518\n",
            "Epoch: 718, Loss Train: 2591834.1789, Accuracy: 0.9035, Specificity: 0.8718, Time: 36.016959\n",
            "Epoch: 719, Loss Train: 2590767.8231, Accuracy: 0.8995, Specificity: 0.8750, Time: 34.912667\n",
            "Epoch: 720, Loss Train: 2589403.9463, Accuracy: 0.9009, Specificity: 0.8772, Time: 35.972758\n",
            "Epoch: 721, Loss Train: 2588165.1000, Accuracy: 0.9024, Specificity: 0.8742, Time: 35.390553\n",
            "Epoch: 722, Loss Train: 2587214.4894, Accuracy: 0.8990, Specificity: 0.8802, Time: 35.960826\n",
            "Epoch: 723, Loss Train: 2586276.4975, Accuracy: 0.9030, Specificity: 0.8685, Time: 36.021912\n",
            "Epoch: 724, Loss Train: 2585116.5875, Accuracy: 0.9002, Specificity: 0.8715, Time: 36.094604\n",
            "Epoch: 725, Loss Train: 2583858.8737, Accuracy: 0.9008, Specificity: 0.8780, Time: 35.857234\n",
            "Epoch: 726, Loss Train: 2582743.8036, Accuracy: 0.9025, Specificity: 0.8692, Time: 36.073248\n",
            "Epoch: 727, Loss Train: 2581770.4491, Accuracy: 0.8997, Specificity: 0.8698, Time: 36.069996\n",
            "Epoch: 728, Loss Train: 2580747.3777, Accuracy: 0.9025, Specificity: 0.8713, Time: 36.006367\n",
            "Epoch: 729, Loss Train: 2579591.3616, Accuracy: 0.9009, Specificity: 0.8740, Time: 35.822384\n",
            "Epoch: 730, Loss Train: 2578414.7884, Accuracy: 0.9009, Specificity: 0.8672, Time: 35.827242\n",
            "Epoch: 731, Loss Train: 2577338.1080, Accuracy: 0.9026, Specificity: 0.8680, Time: 35.924129\n",
            "Epoch: 732, Loss Train: 2576328.9317, Accuracy: 0.9002, Specificity: 0.8812, Time: 36.002887\n",
            "Epoch: 733, Loss Train: 2575275.5493, Accuracy: 0.9025, Specificity: 0.8650, Time: 35.880659\n",
            "Epoch: 734, Loss Train: 2574147.4372, Accuracy: 0.9012, Specificity: 0.8778, Time: 35.837845\n",
            "Epoch: 735, Loss Train: 2573016.2768, Accuracy: 0.9012, Specificity: 0.8720, Time: 35.946940\n",
            "Epoch: 736, Loss Train: 2571945.4062, Accuracy: 0.9025, Specificity: 0.8735, Time: 36.260573\n",
            "Epoch: 737, Loss Train: 2570912.2383, Accuracy: 0.9006, Specificity: 0.8795, Time: 36.086516\n",
            "Epoch: 738, Loss Train: 2569853.8795, Accuracy: 0.9025, Specificity: 0.8693, Time: 36.070894\n",
            "Epoch: 739, Loss Train: 2568752.2623, Accuracy: 0.9015, Specificity: 0.8778, Time: 36.241666\n",
            "Epoch: 740, Loss Train: 2567645.8271, Accuracy: 0.9015, Specificity: 0.8688, Time: 35.906059\n",
            "Epoch: 741, Loss Train: 2566572.6073, Accuracy: 0.9025, Specificity: 0.8690, Time: 36.006831\n",
            "Epoch: 742, Loss Train: 2565525.4338, Accuracy: 0.9011, Specificity: 0.8795, Time: 35.933704\n",
            "Epoch: 743, Loss Train: 2564469.7125, Accuracy: 0.9026, Specificity: 0.8665, Time: 36.169326\n",
            "Epoch: 744, Loss Train: 2563388.8982, Accuracy: 0.9016, Specificity: 0.8795, Time: 35.972910\n",
            "Epoch: 745, Loss Train: 2562298.9441, Accuracy: 0.9019, Specificity: 0.8725, Time: 36.011219\n",
            "Epoch: 746, Loss Train: 2561223.9992, Accuracy: 0.9024, Specificity: 0.8798, Time: 36.129706\n",
            "Epoch: 747, Loss Train: 2560167.9368, Accuracy: 0.9014, Specificity: 0.8750, Time: 35.842812\n",
            "Epoch: 748, Loss Train: 2559114.5407, Accuracy: 0.9026, Specificity: 0.8765, Time: 36.148562\n",
            "Epoch: 749, Loss Train: 2558049.1379, Accuracy: 0.9017, Specificity: 0.8740, Time: 35.882426\n",
            "Epoch: 750, Loss Train: 2556973.4946, Accuracy: 0.9023, Specificity: 0.8668, Time: 35.965494\n",
            "Epoch: 751, Loss Train: 2555900.6708, Accuracy: 0.9023, Specificity: 0.8782, Time: 35.950015\n",
            "Epoch: 752, Loss Train: 2554839.0656, Accuracy: 0.9018, Specificity: 0.8738, Time: 35.989194\n",
            "Epoch: 753, Loss Train: 2553785.0693, Accuracy: 0.9027, Specificity: 0.8632, Time: 36.121146\n",
            "Epoch: 754, Loss Train: 2552729.2415, Accuracy: 0.9019, Specificity: 0.8730, Time: 36.041934\n",
            "Epoch: 755, Loss Train: 2551666.6507, Accuracy: 0.9026, Specificity: 0.8720, Time: 35.800705\n",
            "Epoch: 756, Loss Train: 2550600.7585, Accuracy: 0.9023, Specificity: 0.8832, Time: 36.218297\n",
            "Epoch: 757, Loss Train: 2549538.1656, Accuracy: 0.9024, Specificity: 0.8825, Time: 36.069377\n",
            "Epoch: 758, Loss Train: 2548481.9248, Accuracy: 0.9029, Specificity: 0.8778, Time: 36.232057\n",
            "Epoch: 759, Loss Train: 2547429.4825, Accuracy: 0.9023, Specificity: 0.8840, Time: 36.202744\n",
            "Epoch: 760, Loss Train: 2546376.1880, Accuracy: 0.9031, Specificity: 0.8752, Time: 36.121360\n",
            "Epoch: 761, Loss Train: 2545319.8969, Accuracy: 0.9026, Specificity: 0.8815, Time: 36.336571\n",
            "Epoch: 762, Loss Train: 2544262.1603, Accuracy: 0.9029, Specificity: 0.8818, Time: 35.464736\n",
            "Epoch: 763, Loss Train: 2543206.1894, Accuracy: 0.9030, Specificity: 0.8778, Time: 36.171355\n",
            "Epoch: 764, Loss Train: 2542153.7397, Accuracy: 0.9027, Specificity: 0.8815, Time: 36.055563\n",
            "Epoch: 765, Loss Train: 2541104.0415, Accuracy: 0.9032, Specificity: 0.8790, Time: 36.009308\n",
            "Epoch: 766, Loss Train: 2540055.0025, Accuracy: 0.9027, Specificity: 0.8770, Time: 36.436384\n",
            "Epoch: 767, Loss Train: 2539005.0668, Accuracy: 0.9032, Specificity: 0.8673, Time: 35.902556\n",
            "Epoch: 768, Loss Train: 2537954.3818, Accuracy: 0.9030, Specificity: 0.8718, Time: 35.962324\n",
            "Epoch: 769, Loss Train: 2536904.1934, Accuracy: 0.9032, Specificity: 0.8708, Time: 35.702823\n",
            "Epoch: 770, Loss Train: 2535855.7623, Accuracy: 0.9033, Specificity: 0.8732, Time: 35.875730\n",
            "Epoch: 771, Loss Train: 2534809.4078, Accuracy: 0.9031, Specificity: 0.8735, Time: 36.536571\n",
            "Epoch: 772, Loss Train: 2533764.5254, Accuracy: 0.9035, Specificity: 0.8852, Time: 36.421338\n",
            "Epoch: 773, Loss Train: 2532720.2206, Accuracy: 0.9033, Specificity: 0.8842, Time: 35.993176\n",
            "Epoch: 774, Loss Train: 2531675.9341, Accuracy: 0.9036, Specificity: 0.8733, Time: 36.061648\n",
            "Epoch: 775, Loss Train: 2530631.7788, Accuracy: 0.9035, Specificity: 0.8770, Time: 35.910525\n",
            "Epoch: 776, Loss Train: 2529588.2535, Accuracy: 0.9037, Specificity: 0.8765, Time: 36.193005\n",
            "Epoch: 777, Loss Train: 2528545.8846, Accuracy: 0.9037, Specificity: 0.8747, Time: 36.280339\n",
            "Epoch: 778, Loss Train: 2527504.8769, Accuracy: 0.9036, Specificity: 0.8818, Time: 35.992213\n",
            "Epoch: 779, Loss Train: 2526465.0943, Accuracy: 0.9039, Specificity: 0.8842, Time: 35.249902\n",
            "Epoch: 780, Loss Train: 2525426.2043, Accuracy: 0.9036, Specificity: 0.8912, Time: 36.163214\n",
            "Epoch: 781, Loss Train: 2524387.9049, Accuracy: 0.9039, Specificity: 0.8830, Time: 35.547076\n",
            "Epoch: 782, Loss Train: 2523350.1072, Accuracy: 0.9037, Specificity: 0.8755, Time: 36.113925\n",
            "Epoch: 783, Loss Train: 2522312.8755, Accuracy: 0.9039, Specificity: 0.8772, Time: 36.088374\n",
            "Epoch: 784, Loss Train: 2521276.4130, Accuracy: 0.9038, Specificity: 0.8722, Time: 36.171076\n",
            "Epoch: 785, Loss Train: 2520240.8749, Accuracy: 0.9039, Specificity: 0.8700, Time: 36.321811\n",
            "Epoch: 786, Loss Train: 2519206.3553, Accuracy: 0.9040, Specificity: 0.8822, Time: 36.169983\n",
            "Epoch: 787, Loss Train: 2518172.8303, Accuracy: 0.9039, Specificity: 0.8750, Time: 36.315516\n",
            "Epoch: 788, Loss Train: 2517140.2147, Accuracy: 0.9041, Specificity: 0.8732, Time: 36.275000\n",
            "Epoch: 789, Loss Train: 2516108.4086, Accuracy: 0.9039, Specificity: 0.8740, Time: 36.305958\n",
            "Epoch: 790, Loss Train: 2515077.3306, Accuracy: 0.9042, Specificity: 0.8788, Time: 36.350847\n",
            "Epoch: 791, Loss Train: 2514046.9743, Accuracy: 0.9040, Specificity: 0.8870, Time: 36.128250\n",
            "Epoch: 792, Loss Train: 2513017.3441, Accuracy: 0.9042, Specificity: 0.8858, Time: 36.302446\n",
            "Epoch: 793, Loss Train: 2511988.4961, Accuracy: 0.9041, Specificity: 0.8795, Time: 36.076723\n",
            "Epoch: 794, Loss Train: 2510960.4646, Accuracy: 0.9042, Specificity: 0.8745, Time: 36.074981\n",
            "Epoch: 795, Loss Train: 2509933.2937, Accuracy: 0.9042, Specificity: 0.8825, Time: 36.082867\n",
            "Epoch: 796, Loss Train: 2508906.9895, Accuracy: 0.9042, Specificity: 0.8792, Time: 35.964187\n",
            "Epoch: 797, Loss Train: 2507881.5547, Accuracy: 0.9043, Specificity: 0.8725, Time: 36.033164\n",
            "Epoch: 798, Loss Train: 2506856.9700, Accuracy: 0.9043, Specificity: 0.8652, Time: 36.234661\n",
            "Epoch: 799, Loss Train: 2505833.2207, Accuracy: 0.9044, Specificity: 0.8777, Time: 36.121061\n",
            "Epoch: 800, Loss Train: 2504810.2960, Accuracy: 0.9043, Specificity: 0.8725, Time: 36.234798\n",
            "Epoch: 801, Loss Train: 2503788.1724, Accuracy: 0.9045, Specificity: 0.8712, Time: 36.167589\n",
            "Epoch: 802, Loss Train: 2502766.8512, Accuracy: 0.9044, Specificity: 0.8842, Time: 36.216620\n",
            "Epoch: 803, Loss Train: 2501746.3280, Accuracy: 0.9046, Specificity: 0.8822, Time: 36.390815\n",
            "Epoch: 804, Loss Train: 2500726.6067, Accuracy: 0.9045, Specificity: 0.8870, Time: 35.982302\n",
            "Epoch: 805, Loss Train: 2499707.6881, Accuracy: 0.9047, Specificity: 0.8835, Time: 36.109492\n",
            "Epoch: 806, Loss Train: 2498689.5782, Accuracy: 0.9045, Specificity: 0.8693, Time: 36.094993\n",
            "Epoch: 807, Loss Train: 2497672.2798, Accuracy: 0.9047, Specificity: 0.8845, Time: 35.791259\n",
            "Epoch: 808, Loss Train: 2496655.7963, Accuracy: 0.9046, Specificity: 0.8727, Time: 36.047273\n",
            "Epoch: 809, Loss Train: 2495640.1259, Accuracy: 0.9048, Specificity: 0.8785, Time: 35.909194\n",
            "Epoch: 810, Loss Train: 2494625.2742, Accuracy: 0.9046, Specificity: 0.8688, Time: 36.064946\n",
            "Epoch: 811, Loss Train: 2493611.2350, Accuracy: 0.9048, Specificity: 0.8740, Time: 36.020632\n",
            "Epoch: 812, Loss Train: 2492598.0210, Accuracy: 0.9047, Specificity: 0.8788, Time: 35.623636\n",
            "Epoch: 813, Loss Train: 2491585.6220, Accuracy: 0.9049, Specificity: 0.8738, Time: 35.974936\n",
            "Epoch: 814, Loss Train: 2490574.0432, Accuracy: 0.9048, Specificity: 0.8822, Time: 35.979411\n",
            "Epoch: 815, Loss Train: 2489563.2854, Accuracy: 0.9050, Specificity: 0.8755, Time: 35.908036\n",
            "Epoch: 816, Loss Train: 2488553.3573, Accuracy: 0.9049, Specificity: 0.8825, Time: 36.102069\n",
            "Epoch: 817, Loss Train: 2487544.2630, Accuracy: 0.9052, Specificity: 0.8738, Time: 36.083477\n",
            "Epoch: 818, Loss Train: 2486536.0198, Accuracy: 0.9050, Specificity: 0.8755, Time: 36.122983\n",
            "Epoch: 819, Loss Train: 2485528.6421, Accuracy: 0.9052, Specificity: 0.8880, Time: 36.055185\n",
            "Epoch: 820, Loss Train: 2484522.1721, Accuracy: 0.9050, Specificity: 0.8742, Time: 35.904726\n",
            "Epoch: 821, Loss Train: 2483516.6555, Accuracy: 0.9053, Specificity: 0.8798, Time: 35.951367\n",
            "Epoch: 822, Loss Train: 2482512.2141, Accuracy: 0.9050, Specificity: 0.8850, Time: 35.972925\n",
            "Epoch: 823, Loss Train: 2481508.9872, Accuracy: 0.9055, Specificity: 0.8840, Time: 36.064972\n",
            "Epoch: 824, Loss Train: 2480507.2879, Accuracy: 0.9050, Specificity: 0.8760, Time: 36.157176\n",
            "Epoch: 825, Loss Train: 2479507.5835, Accuracy: 0.9058, Specificity: 0.8695, Time: 35.634573\n",
            "Epoch: 826, Loss Train: 2478510.8114, Accuracy: 0.9048, Specificity: 0.8818, Time: 35.776783\n",
            "Epoch: 827, Loss Train: 2477518.4972, Accuracy: 0.9063, Specificity: 0.8838, Time: 35.519759\n",
            "Epoch: 828, Loss Train: 2476533.6406, Accuracy: 0.9045, Specificity: 0.8738, Time: 36.018234\n",
            "Epoch: 829, Loss Train: 2475561.4387, Accuracy: 0.9069, Specificity: 0.8755, Time: 34.867474\n",
            "Epoch: 830, Loss Train: 2474612.0497, Accuracy: 0.9039, Specificity: 0.8805, Time: 36.044540\n",
            "Epoch: 831, Loss Train: 2473703.6767, Accuracy: 0.9081, Specificity: 0.8805, Time: 36.146363\n",
            "Epoch: 832, Loss Train: 2472872.2948, Accuracy: 0.9026, Specificity: 0.8840, Time: 36.034695\n",
            "Epoch: 833, Loss Train: 2472182.0588, Accuracy: 0.9099, Specificity: 0.8743, Time: 35.679320\n",
            "Epoch: 834, Loss Train: 2471759.5762, Accuracy: 0.9000, Specificity: 0.8915, Time: 35.962104\n",
            "Epoch: 835, Loss Train: 2471810.8855, Accuracy: 0.9130, Specificity: 0.8760, Time: 35.957823\n",
            "Epoch: 836, Loss Train: 2472708.3943, Accuracy: 0.8951, Specificity: 0.8915, Time: 35.959984\n",
            "Epoch: 837, Loss Train: 2474808.4017, Accuracy: 0.9177, Specificity: 0.8662, Time: 36.489793\n",
            "Epoch: 838, Loss Train: 2478361.1404, Accuracy: 0.8897, Specificity: 0.8975, Time: 35.849662\n",
            "Epoch: 839, Loss Train: 2481765.3358, Accuracy: 0.9202, Specificity: 0.8600, Time: 35.851043\n",
            "Epoch: 840, Loss Train: 2482134.9253, Accuracy: 0.8922, Specificity: 0.8995, Time: 35.929833\n",
            "Epoch: 841, Loss Train: 2475671.6220, Accuracy: 0.9126, Specificity: 0.8705, Time: 35.911582\n",
            "Epoch: 842, Loss Train: 2466447.0858, Accuracy: 0.9064, Specificity: 0.8825, Time: 36.037808\n",
            "Epoch: 843, Loss Train: 2461996.5989, Accuracy: 0.8988, Specificity: 0.8875, Time: 36.063131\n",
            "Epoch: 844, Loss Train: 2464612.3699, Accuracy: 0.9162, Specificity: 0.8580, Time: 36.010194\n",
            "Epoch: 845, Loss Train: 2468370.8803, Accuracy: 0.8955, Specificity: 0.8865, Time: 36.223809\n",
            "Epoch: 846, Loss Train: 2466672.2304, Accuracy: 0.9118, Specificity: 0.8700, Time: 35.887541\n",
            "Epoch: 847, Loss Train: 2460870.5826, Accuracy: 0.9066, Specificity: 0.8760, Time: 36.098719\n",
            "Epoch: 848, Loss Train: 2457502.2590, Accuracy: 0.9004, Specificity: 0.8870, Time: 36.104390\n",
            "Epoch: 849, Loss Train: 2459020.0648, Accuracy: 0.9138, Specificity: 0.8715, Time: 36.154499\n",
            "Epoch: 850, Loss Train: 2460785.3874, Accuracy: 0.8991, Specificity: 0.8868, Time: 36.265786\n",
            "Epoch: 851, Loss Train: 2458541.9525, Accuracy: 0.9092, Specificity: 0.8757, Time: 36.136887\n",
            "Epoch: 852, Loss Train: 2454624.1772, Accuracy: 0.9083, Specificity: 0.8792, Time: 36.144929\n",
            "Epoch: 853, Loss Train: 2453396.8194, Accuracy: 0.9007, Specificity: 0.8968, Time: 36.229658\n",
            "Epoch: 854, Loss Train: 2454569.7955, Accuracy: 0.9125, Specificity: 0.8772, Time: 35.917282\n",
            "Epoch: 855, Loss Train: 2454417.1722, Accuracy: 0.9024, Specificity: 0.8815, Time: 36.086911\n",
            "Epoch: 856, Loss Train: 2451825.2826, Accuracy: 0.9069, Specificity: 0.8795, Time: 36.181395\n",
            "Epoch: 857, Loss Train: 2449625.1591, Accuracy: 0.9097, Specificity: 0.8680, Time: 35.977639\n",
            "Epoch: 858, Loss Train: 2449535.1344, Accuracy: 0.9016, Specificity: 0.8855, Time: 36.189419\n",
            "Epoch: 859, Loss Train: 2449800.6995, Accuracy: 0.9109, Specificity: 0.8798, Time: 36.064395\n",
            "Epoch: 860, Loss Train: 2448498.8727, Accuracy: 0.9048, Specificity: 0.8775, Time: 36.174474\n",
            "Epoch: 861, Loss Train: 2446417.8319, Accuracy: 0.9053, Specificity: 0.8808, Time: 36.086468\n",
            "Epoch: 862, Loss Train: 2445420.4675, Accuracy: 0.9102, Specificity: 0.8680, Time: 36.228657\n",
            "Epoch: 863, Loss Train: 2445386.6115, Accuracy: 0.9028, Specificity: 0.8910, Time: 36.235788\n",
            "Epoch: 864, Loss Train: 2444779.4077, Accuracy: 0.9092, Specificity: 0.8852, Time: 36.094428\n",
            "Epoch: 865, Loss Train: 2443236.6364, Accuracy: 0.9065, Specificity: 0.8765, Time: 36.353890\n",
            "Epoch: 866, Loss Train: 2441864.4249, Accuracy: 0.9048, Specificity: 0.8845, Time: 36.310498\n",
            "Epoch: 867, Loss Train: 2441323.5748, Accuracy: 0.9098, Specificity: 0.8690, Time: 36.150140\n",
            "Epoch: 868, Loss Train: 2440920.8057, Accuracy: 0.9041, Specificity: 0.8802, Time: 36.307353\n",
            "Epoch: 869, Loss Train: 2439894.6721, Accuracy: 0.9080, Specificity: 0.8820, Time: 36.253111\n",
            "Epoch: 870, Loss Train: 2438560.5335, Accuracy: 0.9074, Specificity: 0.8707, Time: 36.110694\n",
            "Epoch: 871, Loss Train: 2437625.3253, Accuracy: 0.9048, Specificity: 0.8795, Time: 36.209476\n",
            "Epoch: 872, Loss Train: 2437087.7360, Accuracy: 0.9093, Specificity: 0.8727, Time: 36.116029\n",
            "Epoch: 873, Loss Train: 2436378.9074, Accuracy: 0.9051, Specificity: 0.8748, Time: 36.165753\n",
            "Epoch: 874, Loss Train: 2435287.5186, Accuracy: 0.9073, Specificity: 0.8873, Time: 36.168759\n",
            "Epoch: 875, Loss Train: 2434197.3284, Accuracy: 0.9079, Specificity: 0.8812, Time: 36.115483\n",
            "Epoch: 876, Loss Train: 2433417.5291, Accuracy: 0.9051, Specificity: 0.8870, Time: 36.407839\n",
            "Epoch: 877, Loss Train: 2432765.1048, Accuracy: 0.9089, Specificity: 0.8870, Time: 36.055540\n",
            "Epoch: 878, Loss Train: 2431919.7403, Accuracy: 0.9060, Specificity: 0.8800, Time: 36.385444\n",
            "Epoch: 879, Loss Train: 2430891.6982, Accuracy: 0.9071, Specificity: 0.8750, Time: 35.699632\n",
            "Epoch: 880, Loss Train: 2429945.0897, Accuracy: 0.9081, Specificity: 0.8782, Time: 36.507465\n",
            "Epoch: 881, Loss Train: 2429183.0136, Accuracy: 0.9056, Specificity: 0.8790, Time: 36.377608\n",
            "Epoch: 882, Loss Train: 2428441.6277, Accuracy: 0.9086, Specificity: 0.8787, Time: 36.207300\n",
            "Epoch: 883, Loss Train: 2427564.6554, Accuracy: 0.9065, Specificity: 0.8807, Time: 36.285781\n",
            "Epoch: 884, Loss Train: 2426607.4482, Accuracy: 0.9071, Specificity: 0.8775, Time: 36.155239\n",
            "Epoch: 885, Loss Train: 2425722.9961, Accuracy: 0.9081, Specificity: 0.8775, Time: 36.199954\n",
            "Epoch: 886, Loss Train: 2424940.6731, Accuracy: 0.9061, Specificity: 0.8772, Time: 36.192861\n",
            "Epoch: 887, Loss Train: 2424153.7809, Accuracy: 0.9084, Specificity: 0.8780, Time: 36.267839\n",
            "Epoch: 888, Loss Train: 2423284.5358, Accuracy: 0.9069, Specificity: 0.8850, Time: 36.246122\n",
            "Epoch: 889, Loss Train: 2422375.7559, Accuracy: 0.9074, Specificity: 0.8740, Time: 36.127151\n",
            "Epoch: 890, Loss Train: 2421513.2877, Accuracy: 0.9081, Specificity: 0.8698, Time: 36.422259\n",
            "Epoch: 891, Loss Train: 2420709.5012, Accuracy: 0.9064, Specificity: 0.8850, Time: 36.523553\n",
            "Epoch: 892, Loss Train: 2419903.4099, Accuracy: 0.9083, Specificity: 0.8745, Time: 36.470191\n",
            "Epoch: 893, Loss Train: 2419050.3453, Accuracy: 0.9071, Specificity: 0.8867, Time: 36.553073\n",
            "Epoch: 894, Loss Train: 2418171.7688, Accuracy: 0.9075, Specificity: 0.8855, Time: 36.481881\n",
            "Epoch: 895, Loss Train: 2417316.5868, Accuracy: 0.9080, Specificity: 0.8852, Time: 36.487505\n",
            "Epoch: 896, Loss Train: 2416497.2687, Accuracy: 0.9068, Specificity: 0.8853, Time: 36.577657\n",
            "Epoch: 897, Loss Train: 2415682.9596, Accuracy: 0.9083, Specificity: 0.8723, Time: 36.412113\n",
            "Epoch: 898, Loss Train: 2414844.2339, Accuracy: 0.9072, Specificity: 0.8740, Time: 36.367189\n",
            "Epoch: 899, Loss Train: 2413986.0032, Accuracy: 0.9077, Specificity: 0.8793, Time: 36.510298\n",
            "Epoch: 900, Loss Train: 2413135.1160, Accuracy: 0.9079, Specificity: 0.8870, Time: 36.528668\n",
            "Epoch: 901, Loss Train: 2412305.5664, Accuracy: 0.9072, Specificity: 0.8733, Time: 36.624939\n",
            "Epoch: 902, Loss Train: 2411485.9076, Accuracy: 0.9083, Specificity: 0.8718, Time: 36.809996\n",
            "Epoch: 903, Loss Train: 2410657.0442, Accuracy: 0.9073, Specificity: 0.8940, Time: 36.453772\n",
            "Epoch: 904, Loss Train: 2409814.1706, Accuracy: 0.9080, Specificity: 0.8787, Time: 36.540385\n",
            "Epoch: 905, Loss Train: 2408969.0493, Accuracy: 0.9078, Specificity: 0.8828, Time: 36.528494\n",
            "Epoch: 906, Loss Train: 2408133.9846, Accuracy: 0.9075, Specificity: 0.8840, Time: 36.631859\n",
            "Epoch: 907, Loss Train: 2407309.1073, Accuracy: 0.9083, Specificity: 0.8822, Time: 36.646985\n",
            "Epoch: 908, Loss Train: 2406484.8225, Accuracy: 0.9075, Specificity: 0.8800, Time: 36.483536\n",
            "Epoch: 909, Loss Train: 2405653.4109, Accuracy: 0.9082, Specificity: 0.8753, Time: 36.658228\n",
            "Epoch: 910, Loss Train: 2404816.4192, Accuracy: 0.9079, Specificity: 0.8882, Time: 36.417919\n",
            "Epoch: 911, Loss Train: 2403981.0068, Accuracy: 0.9079, Specificity: 0.8773, Time: 36.392920\n",
            "Epoch: 912, Loss Train: 2403151.9483, Accuracy: 0.9083, Specificity: 0.8865, Time: 36.510648\n",
            "Epoch: 913, Loss Train: 2402327.5576, Accuracy: 0.9078, Specificity: 0.8782, Time: 36.457293\n",
            "Epoch: 914, Loss Train: 2401502.6527, Accuracy: 0.9085, Specificity: 0.8902, Time: 36.478840\n",
            "Epoch: 915, Loss Train: 2400674.1202, Accuracy: 0.9080, Specificity: 0.8780, Time: 36.579056\n",
            "Epoch: 916, Loss Train: 2399843.2075, Accuracy: 0.9084, Specificity: 0.8833, Time: 36.141842\n",
            "Epoch: 917, Loss Train: 2399013.5421, Accuracy: 0.9084, Specificity: 0.8812, Time: 36.478494\n",
            "Epoch: 918, Loss Train: 2398187.3603, Accuracy: 0.9081, Specificity: 0.8858, Time: 36.357261\n",
            "Epoch: 919, Loss Train: 2397363.9042, Accuracy: 0.9086, Specificity: 0.8910, Time: 36.315477\n",
            "Epoch: 920, Loss Train: 2396540.7296, Accuracy: 0.9081, Specificity: 0.8948, Time: 36.371207\n",
            "Epoch: 921, Loss Train: 2395716.0698, Accuracy: 0.9086, Specificity: 0.8875, Time: 36.277870\n",
            "Epoch: 922, Loss Train: 2394890.2003, Accuracy: 0.9083, Specificity: 0.8908, Time: 36.260093\n",
            "Epoch: 923, Loss Train: 2394064.6911, Accuracy: 0.9084, Specificity: 0.8782, Time: 35.755210\n",
            "Epoch: 924, Loss Train: 2393240.9320, Accuracy: 0.9086, Specificity: 0.8850, Time: 36.221296\n",
            "Epoch: 925, Loss Train: 2392419.0556, Accuracy: 0.9083, Specificity: 0.8795, Time: 36.260333\n",
            "Epoch: 926, Loss Train: 2391598.1247, Accuracy: 0.9087, Specificity: 0.8765, Time: 36.471187\n",
            "Epoch: 927, Loss Train: 2390777.0925, Accuracy: 0.9085, Specificity: 0.8753, Time: 36.288644\n",
            "Epoch: 928, Loss Train: 2389955.5483, Accuracy: 0.9087, Specificity: 0.8802, Time: 36.464765\n",
            "Epoch: 929, Loss Train: 2389133.9209, Accuracy: 0.9086, Specificity: 0.8848, Time: 36.222549\n",
            "Epoch: 930, Loss Train: 2388312.9309, Accuracy: 0.9087, Specificity: 0.8798, Time: 36.585469\n",
            "Epoch: 931, Loss Train: 2387493.0794, Accuracy: 0.9088, Specificity: 0.8930, Time: 36.297280\n",
            "Epoch: 932, Loss Train: 2386674.3150, Accuracy: 0.9086, Specificity: 0.8742, Time: 36.326987\n",
            "Epoch: 933, Loss Train: 2385856.2187, Accuracy: 0.9088, Specificity: 0.8830, Time: 36.309151\n",
            "Epoch: 934, Loss Train: 2385038.3553, Accuracy: 0.9086, Specificity: 0.8800, Time: 36.256917\n",
            "Epoch: 935, Loss Train: 2384220.5438, Accuracy: 0.9088, Specificity: 0.8677, Time: 36.340977\n",
            "Epoch: 936, Loss Train: 2383402.9103, Accuracy: 0.9087, Specificity: 0.8855, Time: 35.864154\n",
            "Epoch: 937, Loss Train: 2382585.7428, Accuracy: 0.9088, Specificity: 0.8812, Time: 36.240197\n",
            "Epoch: 938, Loss Train: 2381769.2823, Accuracy: 0.9088, Specificity: 0.8800, Time: 35.928691\n",
            "Epoch: 939, Loss Train: 2380953.5925, Accuracy: 0.9088, Specificity: 0.8870, Time: 36.442425\n",
            "Epoch: 940, Loss Train: 2380138.5694, Accuracy: 0.9090, Specificity: 0.8750, Time: 36.454378\n",
            "Epoch: 941, Loss Train: 2379324.0481, Accuracy: 0.9088, Specificity: 0.8918, Time: 36.520570\n",
            "Epoch: 942, Loss Train: 2378509.8756, Accuracy: 0.9090, Specificity: 0.8790, Time: 36.276997\n",
            "Epoch: 943, Loss Train: 2377696.0238, Accuracy: 0.9089, Specificity: 0.8765, Time: 36.169738\n",
            "Epoch: 944, Loss Train: 2376882.5307, Accuracy: 0.9089, Specificity: 0.8840, Time: 36.320679\n",
            "Epoch: 945, Loss Train: 2376069.5002, Accuracy: 0.9088, Specificity: 0.8915, Time: 36.304825\n",
            "Epoch: 946, Loss Train: 2375257.0173, Accuracy: 0.9089, Specificity: 0.8847, Time: 36.359663\n",
            "Epoch: 947, Loss Train: 2374445.1184, Accuracy: 0.9090, Specificity: 0.8927, Time: 36.240740\n",
            "Epoch: 948, Loss Train: 2373633.7917, Accuracy: 0.9089, Specificity: 0.8925, Time: 36.458268\n",
            "Epoch: 949, Loss Train: 2372822.9930, Accuracy: 0.9090, Specificity: 0.8842, Time: 36.368883\n",
            "Epoch: 950, Loss Train: 2372012.6688, Accuracy: 0.9089, Specificity: 0.8798, Time: 35.952429\n",
            "Epoch: 951, Loss Train: 2371202.7775, Accuracy: 0.9090, Specificity: 0.8812, Time: 36.332946\n",
            "Epoch: 952, Loss Train: 2370393.3138, Accuracy: 0.9089, Specificity: 0.8807, Time: 36.538045\n",
            "Epoch: 953, Loss Train: 2369584.2823, Accuracy: 0.9090, Specificity: 0.8812, Time: 35.091851\n",
            "Epoch: 954, Loss Train: 2368775.7129, Accuracy: 0.9090, Specificity: 0.8818, Time: 36.496465\n",
            "Epoch: 955, Loss Train: 2367967.6210, Accuracy: 0.9091, Specificity: 0.8782, Time: 36.464089\n",
            "Epoch: 956, Loss Train: 2367160.0278, Accuracy: 0.9091, Specificity: 0.8907, Time: 36.269468\n",
            "Epoch: 957, Loss Train: 2366352.9456, Accuracy: 0.9090, Specificity: 0.8777, Time: 36.379389\n",
            "Epoch: 958, Loss Train: 2365546.3609, Accuracy: 0.9091, Specificity: 0.8775, Time: 35.743726\n",
            "Epoch: 959, Loss Train: 2364740.2764, Accuracy: 0.9091, Specificity: 0.8872, Time: 36.494529\n",
            "Epoch: 960, Loss Train: 2363934.6736, Accuracy: 0.9092, Specificity: 0.8982, Time: 36.529353\n",
            "Epoch: 961, Loss Train: 2363129.5498, Accuracy: 0.9092, Specificity: 0.8865, Time: 36.518027\n",
            "Epoch: 962, Loss Train: 2362324.8893, Accuracy: 0.9092, Specificity: 0.8895, Time: 36.714773\n",
            "Epoch: 963, Loss Train: 2361520.6989, Accuracy: 0.9092, Specificity: 0.8825, Time: 36.477203\n",
            "Epoch: 964, Loss Train: 2360716.9676, Accuracy: 0.9093, Specificity: 0.8870, Time: 36.511333\n",
            "Epoch: 965, Loss Train: 2359913.7090, Accuracy: 0.9092, Specificity: 0.8807, Time: 36.741313\n",
            "Epoch: 966, Loss Train: 2359110.9132, Accuracy: 0.9094, Specificity: 0.8842, Time: 36.504776\n",
            "Epoch: 967, Loss Train: 2358308.5963, Accuracy: 0.9093, Specificity: 0.8883, Time: 36.509245\n",
            "Epoch: 968, Loss Train: 2357506.7447, Accuracy: 0.9094, Specificity: 0.8853, Time: 36.262870\n",
            "Epoch: 969, Loss Train: 2356705.3683, Accuracy: 0.9093, Specificity: 0.8827, Time: 36.369351\n",
            "Epoch: 970, Loss Train: 2355904.4679, Accuracy: 0.9094, Specificity: 0.8860, Time: 36.560652\n",
            "Epoch: 971, Loss Train: 2355104.0448, Accuracy: 0.9094, Specificity: 0.8802, Time: 35.778507\n",
            "Epoch: 972, Loss Train: 2354304.0906, Accuracy: 0.9094, Specificity: 0.8820, Time: 36.543054\n",
            "Epoch: 973, Loss Train: 2353504.6138, Accuracy: 0.9094, Specificity: 0.8865, Time: 36.571078\n",
            "Epoch: 974, Loss Train: 2352705.6076, Accuracy: 0.9095, Specificity: 0.8902, Time: 36.611377\n",
            "Epoch: 975, Loss Train: 2351907.0717, Accuracy: 0.9095, Specificity: 0.8838, Time: 36.465321\n",
            "Epoch: 976, Loss Train: 2351109.0139, Accuracy: 0.9096, Specificity: 0.8813, Time: 36.392725\n",
            "Epoch: 977, Loss Train: 2350311.4211, Accuracy: 0.9095, Specificity: 0.8860, Time: 36.492354\n",
            "Epoch: 978, Loss Train: 2349514.3037, Accuracy: 0.9097, Specificity: 0.8820, Time: 36.500489\n",
            "Epoch: 979, Loss Train: 2348717.6567, Accuracy: 0.9097, Specificity: 0.8860, Time: 36.504810\n",
            "Epoch: 980, Loss Train: 2347921.4790, Accuracy: 0.9098, Specificity: 0.8795, Time: 36.471972\n",
            "Epoch: 981, Loss Train: 2347125.7744, Accuracy: 0.9098, Specificity: 0.8808, Time: 36.446945\n",
            "Epoch: 982, Loss Train: 2346330.5322, Accuracy: 0.9098, Specificity: 0.8845, Time: 36.462039\n",
            "Epoch: 983, Loss Train: 2345535.7655, Accuracy: 0.9097, Specificity: 0.8882, Time: 36.441575\n",
            "Epoch: 984, Loss Train: 2344741.4721, Accuracy: 0.9098, Specificity: 0.8795, Time: 36.413643\n",
            "Epoch: 985, Loss Train: 2343947.6514, Accuracy: 0.9098, Specificity: 0.8940, Time: 36.661715\n",
            "Epoch: 986, Loss Train: 2343154.2998, Accuracy: 0.9099, Specificity: 0.8835, Time: 36.494669\n",
            "Epoch: 987, Loss Train: 2342361.4413, Accuracy: 0.9098, Specificity: 0.8818, Time: 35.565036\n",
            "Epoch: 988, Loss Train: 2341569.0683, Accuracy: 0.9100, Specificity: 0.8905, Time: 36.540522\n",
            "Epoch: 989, Loss Train: 2340777.2035, Accuracy: 0.9099, Specificity: 0.8835, Time: 36.476418\n",
            "Epoch: 990, Loss Train: 2339985.8675, Accuracy: 0.9101, Specificity: 0.8828, Time: 36.665726\n",
            "Epoch: 991, Loss Train: 2339195.1228, Accuracy: 0.9098, Specificity: 0.8855, Time: 36.523848\n",
            "Epoch: 992, Loss Train: 2338405.0438, Accuracy: 0.9102, Specificity: 0.8860, Time: 36.628895\n",
            "Epoch: 993, Loss Train: 2337615.7967, Accuracy: 0.9097, Specificity: 0.8940, Time: 36.743359\n",
            "Epoch: 994, Loss Train: 2336827.6534, Accuracy: 0.9103, Specificity: 0.8815, Time: 36.340838\n",
            "Epoch: 995, Loss Train: 2336041.1614, Accuracy: 0.9097, Specificity: 0.8805, Time: 36.622060\n",
            "Epoch: 996, Loss Train: 2335257.2728, Accuracy: 0.9106, Specificity: 0.8825, Time: 36.685285\n",
            "Epoch: 997, Loss Train: 2334477.8845, Accuracy: 0.9093, Specificity: 0.8848, Time: 36.395921\n",
            "Epoch: 998, Loss Train: 2333706.4622, Accuracy: 0.9111, Specificity: 0.8942, Time: 36.586621\n",
            "Epoch: 999, Loss Train: 2332949.9466, Accuracy: 0.9087, Specificity: 0.8765, Time: 36.462056\n",
            "Epoch: 1000, Loss Train: 2332221.3808, Accuracy: 0.9118, Specificity: 0.8880, Time: 36.636149\n",
            "Epoch: 1001, Loss Train: 2331547.3476, Accuracy: 0.9076, Specificity: 0.8835, Time: 36.547859\n",
            "Epoch: 1002, Loss Train: 2330977.9740, Accuracy: 0.9134, Specificity: 0.8742, Time: 36.566929\n",
            "Epoch: 1003, Loss Train: 2330616.9650, Accuracy: 0.9055, Specificity: 0.8910, Time: 36.538224\n",
            "Epoch: 1004, Loss Train: 2330650.8294, Accuracy: 0.9166, Specificity: 0.8835, Time: 36.657055\n",
            "Epoch: 1005, Loss Train: 2331456.6174, Accuracy: 0.9010, Specificity: 0.8975, Time: 36.654356\n",
            "Epoch: 1006, Loss Train: 2333561.7871, Accuracy: 0.9214, Specificity: 0.8700, Time: 36.702160\n",
            "Epoch: 1007, Loss Train: 2337786.6603, Accuracy: 0.8939, Specificity: 0.9007, Time: 36.476266\n",
            "Epoch: 1008, Loss Train: 2343700.8526, Accuracy: 0.9264, Specificity: 0.8622, Time: 36.719486\n",
            "Epoch: 1009, Loss Train: 2349184.3181, Accuracy: 0.8920, Specificity: 0.9020, Time: 36.507863\n",
            "Epoch: 1010, Loss Train: 2346690.4249, Accuracy: 0.9217, Specificity: 0.8778, Time: 36.512458\n",
            "Epoch: 1011, Loss Train: 2335525.2097, Accuracy: 0.9061, Specificity: 0.8920, Time: 36.712228\n",
            "Epoch: 1012, Loss Train: 2324295.4737, Accuracy: 0.9054, Specificity: 0.8842, Time: 36.609348\n",
            "Epoch: 1013, Loss Train: 2324120.0232, Accuracy: 0.9209, Specificity: 0.8750, Time: 37.010503\n",
            "Epoch: 1014, Loss Train: 2331155.3334, Accuracy: 0.8973, Specificity: 0.9015, Time: 36.823994\n",
            "Epoch: 1015, Loss Train: 2332815.7066, Accuracy: 0.9189, Specificity: 0.8778, Time: 36.680798\n",
            "Epoch: 1016, Loss Train: 2325882.3817, Accuracy: 0.9092, Specificity: 0.8923, Time: 36.624465\n",
            "Epoch: 1017, Loss Train: 2319622.6796, Accuracy: 0.9046, Specificity: 0.8902, Time: 36.352209\n",
            "Epoch: 1018, Loss Train: 2321508.8433, Accuracy: 0.9197, Specificity: 0.8685, Time: 36.557553\n",
            "Epoch: 1019, Loss Train: 2325381.7283, Accuracy: 0.9020, Specificity: 0.8893, Time: 36.602880\n",
            "Epoch: 1020, Loss Train: 2322938.0628, Accuracy: 0.9138, Specificity: 0.8822, Time: 36.486572\n",
            "Epoch: 1021, Loss Train: 2317565.2978, Accuracy: 0.9136, Specificity: 0.8780, Time: 36.591598\n",
            "Epoch: 1022, Loss Train: 2316781.4461, Accuracy: 0.9035, Specificity: 0.8993, Time: 36.511114\n",
            "Epoch: 1023, Loss Train: 2319380.0641, Accuracy: 0.9178, Specificity: 0.8782, Time: 36.458461\n",
            "Epoch: 1024, Loss Train: 2318885.6814, Accuracy: 0.9068, Specificity: 0.8938, Time: 36.577659\n",
            "Epoch: 1025, Loss Train: 2315081.0531, Accuracy: 0.9091, Specificity: 0.8865, Time: 36.602203\n",
            "Epoch: 1026, Loss Train: 2313523.4680, Accuracy: 0.9160, Specificity: 0.8745, Time: 36.939965\n",
            "Epoch: 1027, Loss Train: 2314932.8454, Accuracy: 0.9047, Specificity: 0.8952, Time: 36.555121\n",
            "Epoch: 1028, Loss Train: 2314877.7114, Accuracy: 0.9144, Specificity: 0.8730, Time: 36.596057\n",
            "Epoch: 1029, Loss Train: 2312318.2411, Accuracy: 0.9113, Specificity: 0.8900, Time: 36.634101\n",
            "Epoch: 1030, Loss Train: 2310678.1032, Accuracy: 0.9066, Specificity: 0.8895, Time: 36.530404\n",
            "Epoch: 1031, Loss Train: 2311231.2698, Accuracy: 0.9158, Specificity: 0.8840, Time: 36.563131\n",
            "Epoch: 1032, Loss Train: 2311279.9742, Accuracy: 0.9074, Specificity: 0.8870, Time: 36.863053\n",
            "Epoch: 1033, Loss Train: 2309489.7052, Accuracy: 0.9108, Specificity: 0.8882, Time: 36.452672\n",
            "Epoch: 1034, Loss Train: 2307972.3701, Accuracy: 0.9140, Specificity: 0.8930, Time: 36.471759\n",
            "Epoch: 1035, Loss Train: 2307973.0559, Accuracy: 0.9067, Specificity: 0.8918, Time: 36.439053\n",
            "Epoch: 1036, Loss Train: 2307927.4239, Accuracy: 0.9141, Specificity: 0.8865, Time: 36.306869\n",
            "Epoch: 1037, Loss Train: 2306674.3864, Accuracy: 0.9104, Specificity: 0.8853, Time: 36.763813\n",
            "Epoch: 1038, Loss Train: 2305320.7667, Accuracy: 0.9089, Specificity: 0.8930, Time: 36.485106\n",
            "Epoch: 1039, Loss Train: 2304957.1175, Accuracy: 0.9144, Specificity: 0.8825, Time: 36.777697\n",
            "Epoch: 1040, Loss Train: 2304803.0701, Accuracy: 0.9083, Specificity: 0.8872, Time: 36.414238\n",
            "Epoch: 1041, Loss Train: 2303877.7383, Accuracy: 0.9119, Specificity: 0.8915, Time: 36.309849\n",
            "Epoch: 1042, Loss Train: 2302695.3960, Accuracy: 0.9124, Specificity: 0.8815, Time: 36.354289\n",
            "Epoch: 1043, Loss Train: 2302101.9847, Accuracy: 0.9085, Specificity: 0.8835, Time: 36.445507\n",
            "Epoch: 1044, Loss Train: 2301812.8461, Accuracy: 0.9134, Specificity: 0.8865, Time: 36.639636\n",
            "Epoch: 1045, Loss Train: 2301104.9398, Accuracy: 0.9100, Specificity: 0.8825, Time: 35.841089\n",
            "Epoch: 1046, Loss Train: 2300082.9378, Accuracy: 0.9106, Specificity: 0.8990, Time: 36.404257\n",
            "Epoch: 1047, Loss Train: 2299353.5727, Accuracy: 0.9130, Specificity: 0.8835, Time: 36.405054\n",
            "Epoch: 1048, Loss Train: 2298933.6365, Accuracy: 0.9091, Specificity: 0.8842, Time: 36.352403\n",
            "Epoch: 1049, Loss Train: 2298340.1170, Accuracy: 0.9126, Specificity: 0.8888, Time: 36.356367\n",
            "Epoch: 1050, Loss Train: 2297467.9070, Accuracy: 0.9113, Specificity: 0.8895, Time: 36.419991\n",
            "Epoch: 1051, Loss Train: 2296678.8661, Accuracy: 0.9100, Specificity: 0.8872, Time: 36.330513\n",
            "Epoch: 1052, Loss Train: 2296137.5076, Accuracy: 0.9129, Specificity: 0.8845, Time: 36.489891\n",
            "Epoch: 1053, Loss Train: 2295588.0151, Accuracy: 0.9100, Specificity: 0.8912, Time: 36.383335\n",
            "Epoch: 1054, Loss Train: 2294838.4788, Accuracy: 0.9118, Specificity: 0.8912, Time: 36.320528\n",
            "Epoch: 1055, Loss Train: 2294049.8756, Accuracy: 0.9120, Specificity: 0.8865, Time: 36.378861\n",
            "Epoch: 1056, Loss Train: 2293413.6473, Accuracy: 0.9100, Specificity: 0.8953, Time: 36.247655\n",
            "Epoch: 1057, Loss Train: 2292851.9364, Accuracy: 0.9126, Specificity: 0.8895, Time: 36.324931\n",
            "Epoch: 1058, Loss Train: 2292190.3447, Accuracy: 0.9107, Specificity: 0.8927, Time: 36.462798\n",
            "Epoch: 1059, Loss Train: 2291440.8706, Accuracy: 0.9113, Specificity: 0.8870, Time: 36.126024\n",
            "Epoch: 1060, Loss Train: 2290747.9763, Accuracy: 0.9122, Specificity: 0.8933, Time: 36.362428\n",
            "Epoch: 1061, Loss Train: 2290144.3518, Accuracy: 0.9103, Specificity: 0.8858, Time: 36.407497\n",
            "Epoch: 1062, Loss Train: 2289524.7942, Accuracy: 0.9123, Specificity: 0.8767, Time: 35.324777\n",
            "Epoch: 1063, Loss Train: 2288829.7428, Accuracy: 0.9113, Specificity: 0.8955, Time: 33.289030\n",
            "Epoch: 1064, Loss Train: 2288122.6282, Accuracy: 0.9111, Specificity: 0.8912, Time: 32.480205\n",
            "Epoch: 1065, Loss Train: 2287473.4144, Accuracy: 0.9124, Specificity: 0.8745, Time: 33.366715\n",
            "Epoch: 1066, Loss Train: 2286855.7250, Accuracy: 0.9106, Specificity: 0.8898, Time: 34.454136\n",
            "Epoch: 1067, Loss Train: 2286203.9707, Accuracy: 0.9121, Specificity: 0.8870, Time: 33.998111\n",
            "Epoch: 1068, Loss Train: 2285514.8782, Accuracy: 0.9114, Specificity: 0.8832, Time: 32.149921\n",
            "Epoch: 1069, Loss Train: 2284839.1256, Accuracy: 0.9111, Specificity: 0.8897, Time: 32.499885\n",
            "Epoch: 1070, Loss Train: 2284199.6909, Accuracy: 0.9123, Specificity: 0.8767, Time: 32.100346\n",
            "Epoch: 1071, Loss Train: 2283565.3472, Accuracy: 0.9110, Specificity: 0.8852, Time: 35.311288\n",
            "Epoch: 1072, Loss Train: 2282904.5960, Accuracy: 0.9119, Specificity: 0.8910, Time: 32.713145\n",
            "Epoch: 1073, Loss Train: 2282228.9575, Accuracy: 0.9116, Specificity: 0.8855, Time: 31.303527\n",
            "Epoch: 1074, Loss Train: 2281568.3564, Accuracy: 0.9112, Specificity: 0.8955, Time: 35.427069\n",
            "Epoch: 1075, Loss Train: 2280927.3119, Accuracy: 0.9123, Specificity: 0.8890, Time: 37.899655\n",
            "Epoch: 1076, Loss Train: 2280283.9818, Accuracy: 0.9112, Specificity: 0.8902, Time: 36.980929\n",
            "Epoch: 1077, Loss Train: 2279624.3133, Accuracy: 0.9119, Specificity: 0.8848, Time: 34.806206\n",
            "Epoch: 1078, Loss Train: 2278959.0250, Accuracy: 0.9118, Specificity: 0.8910, Time: 35.819484\n",
            "Epoch: 1079, Loss Train: 2278304.4342, Accuracy: 0.9113, Specificity: 0.8890, Time: 36.432695\n",
            "Epoch: 1080, Loss Train: 2277660.4006, Accuracy: 0.9123, Specificity: 0.8932, Time: 37.026074\n",
            "Epoch: 1081, Loss Train: 2277013.8555, Accuracy: 0.9114, Specificity: 0.8838, Time: 36.376990\n",
            "Epoch: 1082, Loss Train: 2276357.8707, Accuracy: 0.9119, Specificity: 0.8893, Time: 34.943289\n",
            "Epoch: 1083, Loss Train: 2275699.1895, Accuracy: 0.9119, Specificity: 0.8837, Time: 35.112273\n",
            "Epoch: 1084, Loss Train: 2275046.9522, Accuracy: 0.9115, Specificity: 0.8877, Time: 36.137127\n",
            "Epoch: 1085, Loss Train: 2274401.0039, Accuracy: 0.9121, Specificity: 0.8888, Time: 36.881638\n",
            "Epoch: 1086, Loss Train: 2273753.9848, Accuracy: 0.9115, Specificity: 0.8928, Time: 35.600018\n",
            "Epoch: 1087, Loss Train: 2273101.6509, Accuracy: 0.9120, Specificity: 0.8853, Time: 35.548704\n",
            "Epoch: 1088, Loss Train: 2272447.3399, Accuracy: 0.9119, Specificity: 0.8825, Time: 34.582847\n",
            "Epoch: 1089, Loss Train: 2271796.4004, Accuracy: 0.9116, Specificity: 0.8822, Time: 33.394676\n",
            "Epoch: 1090, Loss Train: 2271149.5637, Accuracy: 0.9122, Specificity: 0.8778, Time: 33.593659\n",
            "Epoch: 1091, Loss Train: 2270503.0296, Accuracy: 0.9117, Specificity: 0.8822, Time: 34.107767\n",
            "Epoch: 1092, Loss Train: 2269853.7320, Accuracy: 0.9121, Specificity: 0.8940, Time: 33.653697\n",
            "Epoch: 1093, Loss Train: 2269202.7117, Accuracy: 0.9119, Specificity: 0.8830, Time: 33.666397\n",
            "Epoch: 1094, Loss Train: 2268553.0319, Accuracy: 0.9119, Specificity: 0.8850, Time: 35.725006\n",
            "Epoch: 1095, Loss Train: 2267906.0036, Accuracy: 0.9122, Specificity: 0.8872, Time: 37.157709\n",
            "Epoch: 1096, Loss Train: 2267260.0886, Accuracy: 0.9118, Specificity: 0.8858, Time: 37.059536\n",
            "Epoch: 1097, Loss Train: 2266613.1553, Accuracy: 0.9122, Specificity: 0.8923, Time: 37.342427\n",
            "Epoch: 1098, Loss Train: 2265964.9066, Accuracy: 0.9120, Specificity: 0.8948, Time: 37.022270\n",
            "Epoch: 1099, Loss Train: 2265316.7883, Accuracy: 0.9120, Specificity: 0.8870, Time: 37.085829\n",
            "Epoch: 1100, Loss Train: 2264670.1205, Accuracy: 0.9122, Specificity: 0.8815, Time: 37.115903\n",
            "Epoch: 1101, Loss Train: 2264024.7503, Accuracy: 0.9120, Specificity: 0.8920, Time: 36.914433\n",
            "Epoch: 1102, Loss Train: 2263379.5131, Accuracy: 0.9122, Specificity: 0.8787, Time: 37.061468\n",
            "Epoch: 1103, Loss Train: 2262733.6294, Accuracy: 0.9120, Specificity: 0.8740, Time: 37.002946\n",
            "Epoch: 1104, Loss Train: 2262087.3910, Accuracy: 0.9121, Specificity: 0.8925, Time: 36.901917\n",
            "Epoch: 1105, Loss Train: 2261441.6548, Accuracy: 0.9122, Specificity: 0.8810, Time: 36.961487\n",
            "Epoch: 1106, Loss Train: 2260796.8809, Accuracy: 0.9121, Specificity: 0.8877, Time: 36.875716\n",
            "Epoch: 1107, Loss Train: 2260152.7762, Accuracy: 0.9124, Specificity: 0.8848, Time: 37.028121\n",
            "Epoch: 1108, Loss Train: 2259508.7409, Accuracy: 0.9121, Specificity: 0.8910, Time: 37.051189\n",
            "Epoch: 1109, Loss Train: 2258864.4647, Accuracy: 0.9123, Specificity: 0.8960, Time: 36.989143\n",
            "Epoch: 1110, Loss Train: 2258220.1960, Accuracy: 0.9123, Specificity: 0.8895, Time: 37.138364\n",
            "Epoch: 1111, Loss Train: 2257576.3484, Accuracy: 0.9123, Specificity: 0.8875, Time: 37.025265\n",
            "Epoch: 1112, Loss Train: 2256933.1275, Accuracy: 0.9124, Specificity: 0.8905, Time: 37.008048\n",
            "Epoch: 1113, Loss Train: 2256290.3758, Accuracy: 0.9124, Specificity: 0.8930, Time: 36.913858\n",
            "Epoch: 1114, Loss Train: 2255647.7907, Accuracy: 0.9126, Specificity: 0.8875, Time: 36.877847\n",
            "Epoch: 1115, Loss Train: 2255005.2269, Accuracy: 0.9125, Specificity: 0.8845, Time: 37.140293\n",
            "Epoch: 1116, Loss Train: 2254362.7583, Accuracy: 0.9126, Specificity: 0.8905, Time: 37.034180\n",
            "Epoch: 1117, Loss Train: 2253720.6033, Accuracy: 0.9126, Specificity: 0.8840, Time: 37.126557\n",
            "Epoch: 1118, Loss Train: 2253078.8804, Accuracy: 0.9126, Specificity: 0.8795, Time: 36.986784\n",
            "Epoch: 1119, Loss Train: 2252437.5573, Accuracy: 0.9127, Specificity: 0.8927, Time: 36.832050\n",
            "Epoch: 1120, Loss Train: 2251796.5003, Accuracy: 0.9126, Specificity: 0.8885, Time: 36.952250\n",
            "Epoch: 1121, Loss Train: 2251155.5935, Accuracy: 0.9128, Specificity: 0.8928, Time: 37.038167\n",
            "Epoch: 1122, Loss Train: 2250514.8407, Accuracy: 0.9127, Specificity: 0.8970, Time: 36.978961\n",
            "Epoch: 1123, Loss Train: 2249874.3179, Accuracy: 0.9128, Specificity: 0.8853, Time: 37.217768\n",
            "Epoch: 1124, Loss Train: 2249234.1131, Accuracy: 0.9128, Specificity: 0.8875, Time: 36.617030\n",
            "Epoch: 1125, Loss Train: 2248594.2496, Accuracy: 0.9128, Specificity: 0.8915, Time: 36.838661\n",
            "Epoch: 1126, Loss Train: 2247954.6938, Accuracy: 0.9129, Specificity: 0.8898, Time: 37.004986\n",
            "Epoch: 1127, Loss Train: 2247315.3862, Accuracy: 0.9128, Specificity: 0.8913, Time: 36.940078\n",
            "Epoch: 1128, Loss Train: 2246676.2768, Accuracy: 0.9131, Specificity: 0.8865, Time: 37.098940\n",
            "Epoch: 1129, Loss Train: 2246037.3813, Accuracy: 0.9130, Specificity: 0.8955, Time: 36.945550\n",
            "Epoch: 1130, Loss Train: 2245398.7302, Accuracy: 0.9131, Specificity: 0.8838, Time: 37.013350\n",
            "Epoch: 1131, Loss Train: 2244760.3631, Accuracy: 0.9131, Specificity: 0.8928, Time: 36.875163\n",
            "Epoch: 1132, Loss Train: 2244122.2879, Accuracy: 0.9131, Specificity: 0.8905, Time: 37.286292\n",
            "Epoch: 1133, Loss Train: 2243484.4965, Accuracy: 0.9131, Specificity: 0.8870, Time: 37.020387\n",
            "Epoch: 1134, Loss Train: 2242846.9610, Accuracy: 0.9131, Specificity: 0.8945, Time: 36.798103\n",
            "Epoch: 1135, Loss Train: 2242209.6614, Accuracy: 0.9133, Specificity: 0.8815, Time: 37.031757\n",
            "Epoch: 1136, Loss Train: 2241572.5951, Accuracy: 0.9133, Specificity: 0.8965, Time: 37.083531\n",
            "Epoch: 1137, Loss Train: 2240935.7716, Accuracy: 0.9134, Specificity: 0.8870, Time: 36.885990\n",
            "Epoch: 1138, Loss Train: 2240299.2037, Accuracy: 0.9134, Specificity: 0.8882, Time: 36.969098\n",
            "Epoch: 1139, Loss Train: 2239662.9045, Accuracy: 0.9134, Specificity: 0.8925, Time: 37.014246\n",
            "Epoch: 1140, Loss Train: 2239026.8775, Accuracy: 0.9135, Specificity: 0.8885, Time: 37.080164\n",
            "Epoch: 1141, Loss Train: 2238391.1125, Accuracy: 0.9135, Specificity: 0.8875, Time: 37.079653\n",
            "Epoch: 1142, Loss Train: 2237755.5971, Accuracy: 0.9135, Specificity: 0.8902, Time: 36.976314\n",
            "Epoch: 1143, Loss Train: 2237120.3252, Accuracy: 0.9135, Specificity: 0.8930, Time: 37.079739\n",
            "Epoch: 1144, Loss Train: 2236485.3053, Accuracy: 0.9136, Specificity: 0.8838, Time: 36.958642\n",
            "Epoch: 1145, Loss Train: 2235850.5274, Accuracy: 0.9136, Specificity: 0.8805, Time: 36.937322\n",
            "Epoch: 1146, Loss Train: 2235215.9980, Accuracy: 0.9137, Specificity: 0.8860, Time: 36.918594\n",
            "Epoch: 1147, Loss Train: 2234581.7253, Accuracy: 0.9137, Specificity: 0.8990, Time: 36.982237\n",
            "Epoch: 1148, Loss Train: 2233947.7111, Accuracy: 0.9137, Specificity: 0.8840, Time: 37.246449\n",
            "Epoch: 1149, Loss Train: 2233313.9464, Accuracy: 0.9137, Specificity: 0.8815, Time: 37.139337\n",
            "Epoch: 1150, Loss Train: 2232680.4427, Accuracy: 0.9137, Specificity: 0.8890, Time: 36.843749\n",
            "Epoch: 1151, Loss Train: 2232047.1866, Accuracy: 0.9138, Specificity: 0.8870, Time: 37.188397\n",
            "Epoch: 1152, Loss Train: 2231414.1816, Accuracy: 0.9138, Specificity: 0.8878, Time: 36.982142\n",
            "Epoch: 1153, Loss Train: 2230781.4227, Accuracy: 0.9138, Specificity: 0.8972, Time: 36.962777\n",
            "Epoch: 1154, Loss Train: 2230148.9100, Accuracy: 0.9138, Specificity: 0.8928, Time: 36.805183\n",
            "Epoch: 1155, Loss Train: 2229516.6438, Accuracy: 0.9139, Specificity: 0.8902, Time: 36.820585\n",
            "Epoch: 1156, Loss Train: 2228884.6283, Accuracy: 0.9139, Specificity: 0.8883, Time: 37.069697\n",
            "Epoch: 1157, Loss Train: 2228252.8614, Accuracy: 0.9139, Specificity: 0.8950, Time: 36.865496\n",
            "Epoch: 1158, Loss Train: 2227621.3473, Accuracy: 0.9139, Specificity: 0.8895, Time: 36.263955\n",
            "Epoch: 1159, Loss Train: 2226990.0803, Accuracy: 0.9139, Specificity: 0.8908, Time: 36.872759\n",
            "Epoch: 1160, Loss Train: 2226359.0652, Accuracy: 0.9140, Specificity: 0.8962, Time: 36.766669\n",
            "Epoch: 1161, Loss Train: 2225728.2964, Accuracy: 0.9140, Specificity: 0.8948, Time: 37.107636\n",
            "Epoch: 1162, Loss Train: 2225097.7772, Accuracy: 0.9140, Specificity: 0.8875, Time: 36.982451\n",
            "Epoch: 1163, Loss Train: 2224467.4996, Accuracy: 0.9140, Specificity: 0.8943, Time: 36.713401\n",
            "Epoch: 1164, Loss Train: 2223837.4744, Accuracy: 0.9140, Specificity: 0.8933, Time: 37.015115\n",
            "Epoch: 1165, Loss Train: 2223207.6950, Accuracy: 0.9141, Specificity: 0.8970, Time: 36.864199\n",
            "Epoch: 1166, Loss Train: 2222578.1598, Accuracy: 0.9141, Specificity: 0.8913, Time: 36.958740\n",
            "Epoch: 1167, Loss Train: 2221948.8686, Accuracy: 0.9141, Specificity: 0.8980, Time: 37.097087\n",
            "Epoch: 1168, Loss Train: 2221319.8280, Accuracy: 0.9142, Specificity: 0.8918, Time: 36.829148\n",
            "Epoch: 1169, Loss Train: 2220691.0328, Accuracy: 0.9142, Specificity: 0.8908, Time: 36.937424\n",
            "Epoch: 1170, Loss Train: 2220062.4824, Accuracy: 0.9143, Specificity: 0.8910, Time: 36.516883\n",
            "Epoch: 1171, Loss Train: 2219434.1705, Accuracy: 0.9143, Specificity: 0.8915, Time: 36.714103\n",
            "Epoch: 1172, Loss Train: 2218806.1099, Accuracy: 0.9143, Specificity: 0.8962, Time: 36.796774\n",
            "Epoch: 1173, Loss Train: 2218178.2945, Accuracy: 0.9143, Specificity: 0.8900, Time: 36.784053\n",
            "Epoch: 1174, Loss Train: 2217550.7188, Accuracy: 0.9144, Specificity: 0.8912, Time: 36.972739\n",
            "Epoch: 1175, Loss Train: 2216923.3890, Accuracy: 0.9144, Specificity: 0.8838, Time: 36.942688\n",
            "Epoch: 1176, Loss Train: 2216296.3059, Accuracy: 0.9144, Specificity: 0.8980, Time: 36.561132\n",
            "Epoch: 1177, Loss Train: 2215669.4586, Accuracy: 0.9144, Specificity: 0.8928, Time: 36.803807\n",
            "Epoch: 1178, Loss Train: 2215042.8610, Accuracy: 0.9144, Specificity: 0.8915, Time: 36.804627\n",
            "Epoch: 1179, Loss Train: 2214416.5066, Accuracy: 0.9145, Specificity: 0.8918, Time: 36.743514\n",
            "Epoch: 1180, Loss Train: 2213790.3953, Accuracy: 0.9144, Specificity: 0.8917, Time: 36.928226\n",
            "Epoch: 1181, Loss Train: 2213164.5254, Accuracy: 0.9144, Specificity: 0.8918, Time: 36.886780\n",
            "Epoch: 1182, Loss Train: 2212538.8992, Accuracy: 0.9145, Specificity: 0.8860, Time: 36.918173\n",
            "Epoch: 1183, Loss Train: 2211913.5147, Accuracy: 0.9145, Specificity: 0.8947, Time: 36.824124\n",
            "Epoch: 1184, Loss Train: 2211288.3699, Accuracy: 0.9146, Specificity: 0.8912, Time: 36.869181\n",
            "Epoch: 1185, Loss Train: 2210663.4697, Accuracy: 0.9146, Specificity: 0.8918, Time: 37.025455\n",
            "Epoch: 1186, Loss Train: 2210038.8140, Accuracy: 0.9146, Specificity: 0.8877, Time: 36.701246\n",
            "Epoch: 1187, Loss Train: 2209414.3960, Accuracy: 0.9146, Specificity: 0.8880, Time: 36.840217\n",
            "Epoch: 1188, Loss Train: 2208790.2225, Accuracy: 0.9146, Specificity: 0.8965, Time: 37.069956\n",
            "Epoch: 1189, Loss Train: 2208166.2880, Accuracy: 0.9146, Specificity: 0.8837, Time: 36.286727\n",
            "Epoch: 1190, Loss Train: 2207542.6024, Accuracy: 0.9146, Specificity: 0.8938, Time: 36.910239\n",
            "Epoch: 1191, Loss Train: 2206919.1546, Accuracy: 0.9146, Specificity: 0.8838, Time: 36.645103\n",
            "Epoch: 1192, Loss Train: 2206295.9595, Accuracy: 0.9146, Specificity: 0.8957, Time: 36.975501\n",
            "Epoch: 1193, Loss Train: 2205673.0147, Accuracy: 0.9146, Specificity: 0.8948, Time: 36.963023\n",
            "Epoch: 1194, Loss Train: 2205050.3287, Accuracy: 0.9147, Specificity: 0.8808, Time: 36.866214\n",
            "Epoch: 1195, Loss Train: 2204427.9107, Accuracy: 0.9146, Specificity: 0.9002, Time: 36.954495\n",
            "Epoch: 1196, Loss Train: 2203805.7908, Accuracy: 0.9147, Specificity: 0.8895, Time: 36.855362\n",
            "Epoch: 1197, Loss Train: 2203184.0098, Accuracy: 0.9145, Specificity: 0.8945, Time: 36.961178\n",
            "Epoch: 1198, Loss Train: 2202562.6393, Accuracy: 0.9149, Specificity: 0.8942, Time: 37.186731\n",
            "Epoch: 1199, Loss Train: 2201941.8327, Accuracy: 0.9146, Specificity: 0.8860, Time: 36.833283\n",
            "Epoch: 1200, Loss Train: 2201321.8285, Accuracy: 0.9151, Specificity: 0.8935, Time: 37.002725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure();\n",
        "ax = fig.add_subplot();\n",
        "\n",
        "plt.plot(SP,AC,linewidth = '2');\n",
        "plt.plot([0,1],[0,1],linewidth = '1',linestyle='dashed');\n",
        "plt.xlim([0, 1]);\n",
        "plt.ylim([0, 1]);\n",
        "ax.set_aspect('equal', adjustable='box');\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('DistMULT NSCE\\nembedding_dim=250, c_neg=None, regularization=0.002\\n')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "JyDQhaPPKeGQ",
        "outputId": "40bcd74a-5696-473b-9ab7-ab044aadc152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(np.array(AC[:-1]) * np.array([SP[i] - SP[i+1] for i in range(len(SP)-1)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCX8d6vJKj9B",
        "outputId": "fe1d4934-1712-4a43-edb8-e80f55fec5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9662019511242417"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUROC = 0.9662019511242417"
      ],
      "metadata": {
        "id": "NgDs4fNQkZ_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}